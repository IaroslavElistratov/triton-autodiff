module {
  tt.func public @add_kernel(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg3: !tt.ptr<f32>, %arg4: !tt.ptr<f32>, %arg5: !tt.ptr<f32>) attributes {noinline = false} {
    %1 = tt.make_range {autogradVisited = true, end = 4 : i32, isCloned = true, start = 0 : i32} : tensor<4xi32>

    // NOTE: this %2 was used in store(%2, %10) -- but bc
    //  I'm currently deleting that store (previously was needed
    //  as to not overwrite the upstream), this %2 is not used in fwd
    %0 = tt.splat %arg2 {autogradVisited = true, isCloned = true} : !tt.ptr<f32> -> tensor<4x!tt.ptr<f32>>
    %2 = tt.addptr %0, %1 {autogradVisited = true, isCloned = true} : tensor<4x!tt.ptr<f32>>, tensor<4xi32>

    // load a
    %3 = tt.splat %arg0 {autogradVisited = true, isCloned = true} : !tt.ptr<f32> -> tensor<4x!tt.ptr<f32>>
    %4 = tt.addptr %3, %1 {autogradVisited = true, isCloned = true} : tensor<4x!tt.ptr<f32>>, tensor<4xi32>
    %5 = tt.load %4 {autogradVisited = true, isCloned = true} : tensor<4x!tt.ptr<f32>>

    // x = a * 0.5
    %cst = arith.constant {autogradVisited = true, isCloned = true} dense<5.000000e-01> : tensor<4xf32>
    %6 = arith.addf %5, %cst {autogradVisited = true, isCloned = true} : tensor<4xf32>

    // load b
    %7 = tt.splat %arg1 {autogradVisited = true, isCloned = true} : !tt.ptr<f32> -> tensor<4x!tt.ptr<f32>>
    %8 = tt.addptr %7, %1 {autogradVisited = true, isCloned = true} : tensor<4x!tt.ptr<f32>>, tensor<4xi32>
    %9 = tt.load %8 {autogradVisited = true, isCloned = true} : tensor<4x!tt.ptr<f32>>

    // y = x * b
    %10 = arith.mulf %6, %9 {autogradVisited = true, isCloned = true} : tensor<4xf32>

    /////////////
    // backward
    /////////////

    // load upstream
    %11 = tt.splat %arg5 {autogradVisited = true, isGradPtrRebase = true} : !tt.ptr<f32> -> tensor<4x!tt.ptr<f32>>
    %12 = tt.addptr %11, %1 {autogradVisited = true, isGradPtrRebase = true} : tensor<4x!tt.ptr<f32>>, tensor<4xi32>
    %13 = tt.load %12 {autogradVisited = true, isInserted = true} : tensor<4x!tt.ptr<f32>>

    // grad b (x * upstream)
    %14 = arith.mulf %6, %13 {autogradVisited = true, isInserted = true} : tensor<4xf32>
    // grad x (b * upstream)
    %15 = arith.mulf %9, %13 {autogradVisited = true, isInserted = true} : tensor<4xf32>

    // store b grad
    %16 = tt.splat %arg4 {autogradVisited = true, isGradPtrRebase = true} : !tt.ptr<f32> -> tensor<4x!tt.ptr<f32>>
    %17 = tt.addptr %16, %1 {autogradVisited = true, isGradPtrRebase = true} : tensor<4x!tt.ptr<f32>>, tensor<4xi32>
    %18 = tt.atomic_rmw fadd, acq_rel, gpu, %17, %14 {autogradVisited = true, gradOf = "%22 = tt.load %21 : tensor<4x!tt.ptr<f32>>", isInserted = true} : (tensor<4x!tt.ptr<f32>>, tensor<4xf32>) -> tensor<4xf32>
    // store a grad (in this case a grad == x grad, because grad of add is one)
    %19 = tt.splat %arg3 {autogradVisited = true, isGradPtrRebase = true} : !tt.ptr<f32> -> tensor<4x!tt.ptr<f32>>
    %20 = tt.addptr %19, %1 {autogradVisited = true, isGradPtrRebase = true} : tensor<4x!tt.ptr<f32>>, tensor<4xi32>
    %21 = tt.atomic_rmw fadd, acq_rel, gpu, %20, %15 {autogradVisited = true, gradOf = "%19 = tt.load %18 : tensor<4x!tt.ptr<f32>>", isInserted = true} : (tensor<4x!tt.ptr<f32>>, tensor<4xf32>) -> tensor<4xf32>
    tt.return
  }
}

