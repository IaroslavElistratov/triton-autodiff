module {
  tt.func public @matmul_kernel(%arg0_A: !tt.ptr<f16> {tt.divisibility = 16 : i32}, %arg1_B: !tt.ptr<f16> {tt.divisibility = 16 : i32}, %arg2_OUT: !tt.ptr<f16> {tt.divisibility = 16 : i32}, %arg3_GRAD_A: !tt.ptr<f16>, %arg4_GRAD_B: !tt.ptr<f16>, %arg5_GRAD_OUT: !tt.ptr<f16>) attributes {noinline = false} {
    %0 = tt.splat %arg2_OUT {autogradVisited = true, isCloned = true} : !tt.ptr<f16> -> tensor<16x1x!tt.ptr<f16>>
    %1 = tt.get_program_id x {autogradVisited = true, isCloned = true} : i32
    %c2_i32 = arith.constant {autogradVisited = true, isCloned = true} 2 : i32
    %2 = arith.divsi %1, %c2_i32 {autogradVisited = true, isCloned = true} : i32
    %c16_i32 = arith.constant {autogradVisited = true, isCloned = true} 16 : i32
    %3 = arith.muli %2, %c16_i32 {autogradVisited = true, isCloned = true} : i32
    %4 = tt.splat %3 {autogradVisited = true, isCloned = true} : i32 -> tensor<16xi32>
    %5 = tt.make_range {autogradVisited = true, end = 16 : i32, isCloned = true, start = 0 : i32} : tensor<16xi32>
    %6 = arith.addi %4, %5 {autogradVisited = true, isCloned = true} : tensor<16xi32>
    %7 = tt.expand_dims %6 {autogradVisited = true, axis = 1 : i32, isCloned = true} : tensor<16xi32> -> tensor<16x1xi32>
    %cst = arith.constant {autogradVisited = true, isCloned = true} dense<32> : tensor<16x1xi32>
    %8 = arith.muli %7, %cst {autogradVisited = true, isCloned = true} : tensor<16x1xi32>
    %9 = tt.addptr %0, %8 {autogradVisited = true, isCloned = true} : tensor<16x1x!tt.ptr<f16>>, tensor<16x1xi32>
    %10 = tt.broadcast %9 {autogradVisited = true, isCloned = true} : tensor<16x1x!tt.ptr<f16>> -> tensor<16x16x!tt.ptr<f16>>
    %11 = arith.remsi %1, %c2_i32 {autogradVisited = true, isCloned = true} : i32
    %12 = arith.muli %11, %c16_i32 {autogradVisited = true, isCloned = true} : i32
    %13 = tt.splat %12 {autogradVisited = true, isCloned = true} : i32 -> tensor<16xi32>
    %14 = arith.addi %13, %5 {autogradVisited = true, isCloned = true} : tensor<16xi32>
    %15 = tt.expand_dims %14 {autogradVisited = true, axis = 0 : i32, isCloned = true} : tensor<16xi32> -> tensor<1x16xi32>
    %16 = tt.broadcast %15 {autogradVisited = true, isCloned = true} : tensor<1x16xi32> -> tensor<16x16xi32>
    %17 = tt.addptr %10, %16 {autogradVisited = true, isCloned = true} : tensor<16x16x!tt.ptr<f16>>, tensor<16x16xi32>
    %18 = tt.splat %arg0_A {autogradVisited = true, isCloned = true} : !tt.ptr<f16> -> tensor<16x16x!tt.ptr<f16>>
    %cst_0 = arith.constant {autogradVisited = true, isCloned = true} dense<32> : tensor<16xi32>
    %19 = arith.remsi %6, %cst_0 {autogradVisited = true, isCloned = true} : tensor<16xi32>
    %20 = tt.expand_dims %19 {autogradVisited = true, axis = 1 : i32, isCloned = true} : tensor<16xi32> -> tensor<16x1xi32>
    %21 = arith.muli %20, %cst {autogradVisited = true, isCloned = true} : tensor<16x1xi32>
    %22 = tt.broadcast %21 {autogradVisited = true, isCloned = true} : tensor<16x1xi32> -> tensor<16x16xi32>
    %23 = tt.expand_dims %5 {autogradVisited = true, axis = 0 : i32, isCloned = true} : tensor<16xi32> -> tensor<1x16xi32>
    %24 = tt.broadcast %23 {autogradVisited = true, isCloned = true} : tensor<1x16xi32> -> tensor<16x16xi32>
    %25 = arith.addi %22, %24 {autogradVisited = true, isCloned = true} : tensor<16x16xi32>
    %26 = tt.addptr %18, %25 {autogradVisited = true, isCloned = true} : tensor<16x16x!tt.ptr<f16>>, tensor<16x16xi32>
    %cst_1 = arith.constant {autogradVisited = true, isCloned = true} dense<16> : tensor<16x16xi32>
    %27 = tt.addptr %26, %cst_1 {autogradVisited = true, isCloned = true} : tensor<16x16x!tt.ptr<f16>>, tensor<16x16xi32>
    %28 = tt.load %27 {autogradVisited = true, isCloned = true} : tensor<16x16x!tt.ptr<f16>>
    %29 = tt.splat %arg1_B {autogradVisited = true, isCloned = true} : !tt.ptr<f16> -> tensor<16x16x!tt.ptr<f16>>
    %30 = tt.expand_dims %5 {autogradVisited = true, axis = 1 : i32, isCloned = true} : tensor<16xi32> -> tensor<16x1xi32>
    %31 = arith.muli %30, %cst {autogradVisited = true, isCloned = true} : tensor<16x1xi32>
    %32 = tt.broadcast %31 {autogradVisited = true, isCloned = true} : tensor<16x1xi32> -> tensor<16x16xi32>
    %33 = arith.remsi %14, %cst_0 {autogradVisited = true, isCloned = true} : tensor<16xi32>
    %34 = tt.expand_dims %33 {autogradVisited = true, axis = 0 : i32, isCloned = true} : tensor<16xi32> -> tensor<1x16xi32>
    %35 = tt.broadcast %34 {autogradVisited = true, isCloned = true} : tensor<1x16xi32> -> tensor<16x16xi32>
    %36 = arith.addi %32, %35 {autogradVisited = true, isCloned = true} : tensor<16x16xi32>
    %37 = tt.addptr %29, %36 {autogradVisited = true, isCloned = true} : tensor<16x16x!tt.ptr<f16>>, tensor<16x16xi32>
    %cst_2 = arith.constant {autogradVisited = true, isCloned = true} dense<512> : tensor<16x16xi32>
    %38 = tt.addptr %37, %cst_2 {autogradVisited = true, isCloned = true} : tensor<16x16x!tt.ptr<f16>>, tensor<16x16xi32>
    %39 = tt.load %38 {autogradVisited = true, isCloned = true} : tensor<16x16x!tt.ptr<f16>>
    %40 = tt.load %26 {autogradVisited = true, isCloned = true} : tensor<16x16x!tt.ptr<f16>>
    %41 = tt.load %37 {autogradVisited = true, isCloned = true} : tensor<16x16x!tt.ptr<f16>>
    %cst_3 = arith.constant {autogradVisited = true, isCloned = true} dense<0.000000e+00> : tensor<16x16xf32>
    // out1 = a(1) @ b(1)
    %42 = tt.dot %40, %41, %cst_3, inputPrecision = tf32 {autogradVisited = true, isCloned = true} : tensor<16x16xf16> * tensor<16x16xf16> -> tensor<16x16xf32>
    // out2 = a(2) @ b(2)
    %43 = tt.dot %28, %39, %42, inputPrecision = tf32 {autogradVisited = true, isCloned = true} : tensor<16x16xf16> * tensor<16x16xf16> -> tensor<16x16xf32>
    %44 = arith.truncf %43 {autogradVisited = true, isCloned = true} : tensor<16x16xf32> to tensor<16x16xf16>


    // load grad_out
    %45 = tt.splat %arg5_GRAD_OUT {autogradVisited = true, isGradPtrRebase = true} : !tt.ptr<f16> -> tensor<16x1x!tt.ptr<f16>>
    %46 = tt.addptr %45, %8 {autogradVisited = true, isGradPtrRebase = true} : tensor<16x1x!tt.ptr<f16>>, tensor<16x1xi32>
    %47 = tt.broadcast %46 {autogradVisited = true, isGradPtrRebase = true} : tensor<16x1x!tt.ptr<f16>> -> tensor<16x16x!tt.ptr<f16>>
    %48 = tt.addptr %47, %16 {autogradVisited = true, isGradPtrRebase = true} : tensor<16x16x!tt.ptr<f16>>, tensor<16x16xi32>
    %49 = tt.load %48 {autogradVisited = true, isInserted = true} : tensor<16x16x!tt.ptr<f16>>


    // todo-now:
    //  1) backprops though 1st iteration before 2nd (incorrect order)
    //  2) uses grad_out as upstream for backprop'ing through both matmuls in different iteations -- incorrent bc when backprop'ing though 1'st mamtul the upstream there is not grad_out, but downstream grad of 2nd matmul

    ////// backprop though 1st itter's mamtul ///////

    // b(1).trans
    %50 = tt.trans %41 {autogradVisited = true, isInserted = true, order = array<i32: 1, 0>} : tensor<16x16xf16> -> tensor<16x16xf16>
    // init accum zeros
    %cst_4 = arith.constant {autogradVisited = true, isInserted = true} 0.000000e+00 : f16
    %51 = tt.splat %cst_4 {autogradVisited = true, isInserted = true} : f16 -> tensor<16x16xf16>
    // a_grad(1) = grad_out @ b(1).trans
    %52 = tt.dot %49, %50, %51, inputPrecision = tf32 {autogradVisited = true, isInserted = true} : tensor<16x16xf16> * tensor<16x16xf16> -> tensor<16x16xf16>

    // a(1).trans
    %53 = tt.trans %40 {autogradVisited = true, isInserted = true, order = array<i32: 1, 0>} : tensor<16x16xf16> -> tensor<16x16xf16>
    // init accum zeros
    %cst_5 = arith.constant {autogradVisited = true, isInserted = true} 0.000000e+00 : f16
    %54 = tt.splat %cst_5 {autogradVisited = true, isInserted = true} : f16 -> tensor<16x16xf16>
    // b_grad(1) = a(1).trans @ grad_out
    %55 = tt.dot %53, %49, %54, inputPrecision = tf32 {autogradVisited = true, isInserted = true} : tensor<16x16xf16> * tensor<16x16xf16> -> tensor<16x16xf16>


    ////// backprop though 2nd itter's mamtul ///////

    // b(2).trans
    %56 = tt.trans %39 {autogradVisited = true, isInserted = true, order = array<i32: 1, 0>} : tensor<16x16xf16> -> tensor<16x16xf16>
    // init accum zeros
    %cst_6 = arith.constant {autogradVisited = true, isInserted = true} 0.000000e+00 : f16
    %57 = tt.splat %cst_6 {autogradVisited = true, isInserted = true} : f16 -> tensor<16x16xf16>
    // a_grad(2) = upstream @ b(2).trans
    %58 = tt.dot %49, %56, %57, inputPrecision = tf32 {autogradVisited = true, isInserted = true} : tensor<16x16xf16> * tensor<16x16xf16> -> tensor<16x16xf16>

    // a(2).trans
    %59 = tt.trans %28 {autogradVisited = true, isInserted = true, order = array<i32: 1, 0>} : tensor<16x16xf16> -> tensor<16x16xf16>
    // init accum zeros
    %cst_7 = arith.constant {autogradVisited = true, isInserted = true} 0.000000e+00 : f16
    %60 = tt.splat %cst_7 {autogradVisited = true, isInserted = true} : f16 -> tensor<16x16xf16>
    // b_grad(2) = a(2).trans @ upstream
    %61 = tt.dot %59, %49, %60, inputPrecision = tf32 {autogradVisited = true, isInserted = true} : tensor<16x16xf16> * tensor<16x16xf16> -> tensor<16x16xf16>


    // store

    // store grad_b(1)
    %62 = tt.splat %arg4_GRAD_B {autogradVisited = true, isGradPtrRebase = true} : !tt.ptr<f16> -> tensor<16x16x!tt.ptr<f16>>
    %63 = tt.addptr %62, %36 {autogradVisited = true, isGradPtrRebase = true} : tensor<16x16x!tt.ptr<f16>>, tensor<16x16xi32>
    %64 = tt.atomic_rmw fadd, acq_rel, gpu, %63, %55 {autogradVisited = true, gradOf = "%91 = tt.load %89 : tensor<16x16x!tt.ptr<f16>>", isInserted = true} : (tensor<16x16x!tt.ptr<f16>>, tensor<16x16xf16>) -> tensor<16x16xf16>

    // store grad_b(2)
    %65 = tt.splat %arg4_GRAD_B {autogradVisited = true, isGradPtrRebase = true} : !tt.ptr<f16> -> tensor<16x16x!tt.ptr<f16>>
    %66 = tt.addptr %65, %36 {autogradVisited = true, isGradPtrRebase = true} : tensor<16x16x!tt.ptr<f16>>, tensor<16x16xi32>
    %67 = tt.addptr %66, %cst_2 {autogradVisited = true, isGradPtrRebase = true} : tensor<16x16x!tt.ptr<f16>>, tensor<16x16xi32>
    %68 = tt.atomic_rmw fadd, acq_rel, gpu, %67, %61 {autogradVisited = true, gradOf = "%96 = tt.load %94 : tensor<16x16x!tt.ptr<f16>>", isInserted = true} : (tensor<16x16x!tt.ptr<f16>>, tensor<16x16xf16>) -> tensor<16x16xf16>

    // store grad_a(1)
    %69 = tt.splat %arg3_GRAD_A {autogradVisited = true, isGradPtrRebase = true} : !tt.ptr<f16> -> tensor<16x16x!tt.ptr<f16>>
    %70 = tt.addptr %69, %25 {autogradVisited = true, isGradPtrRebase = true} : tensor<16x16x!tt.ptr<f16>>, tensor<16x16xi32>
    %71 = tt.atomic_rmw fadd, acq_rel, gpu, %70, %52 {autogradVisited = true, gradOf = "%90 = tt.load %81 : tensor<16x16x!tt.ptr<f16>>", isInserted = true} : (tensor<16x16x!tt.ptr<f16>>, tensor<16x16xf16>) -> tensor<16x16xf16>

    // store grad_a(2)
    %72 = tt.splat %arg3_GRAD_A {autogradVisited = true, isGradPtrRebase = true} : !tt.ptr<f16> -> tensor<16x16x!tt.ptr<f16>>
    %73 = tt.addptr %72, %25 {autogradVisited = true, isGradPtrRebase = true} : tensor<16x16x!tt.ptr<f16>>, tensor<16x16xi32>
    %74 = tt.addptr %73, %cst_1 {autogradVisited = true, isGradPtrRebase = true} : tensor<16x16x!tt.ptr<f16>>, tensor<16x16xi32>
    %75 = tt.atomic_rmw fadd, acq_rel, gpu, %74, %58 {autogradVisited = true, gradOf = "%95 = tt.load %93 : tensor<16x16x!tt.ptr<f16>>", isInserted = true} : (tensor<16x16x!tt.ptr<f16>>, tensor<16x16xf16>) -> tensor<16x16xf16>

    tt.return
  }
}

