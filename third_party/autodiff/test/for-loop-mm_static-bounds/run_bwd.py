import numpy as np
import torch
DEVICE = torch.device("cuda:0")

import triton
from triton.compiler import compile
from triton.backends.compiler import GPUTarget


# Create a GPU target
target = GPUTarget("cuda", arch=89, warp_size=32)

# Compile the IR
bwd_kernel = compile("out.ttir", target=target)

def bwd(
      a,
      b,
      BLOCK_SIZE_M=16,
      BLOCK_SIZE_N=16,
      BLOCK_SIZE_K=16
    ):
    # Check constraints.
    assert a.shape[1] == b.shape[0], "Incompatible dimensions"
    assert a.is_contiguous(), "Matrix A must be contiguous"
    M, K = a.shape
    K, N = b.shape
    out = torch.empty((M, N), device=a.device, dtype=torch.float16)
    # 1D launch kernel where each block gets its own program.
    grid = (triton.cdiv(M, BLOCK_SIZE_M) * triton.cdiv(N, BLOCK_SIZE_N), 1, 1)

    # because I now add additional arguments for each input ptr, to represent grad ptr
    grad_a = torch.zeros_like(a)
    grad_b = torch.zeros_like(b)
    # upstream
    grad_out = torch.ones((M, N), device=a.device, dtype=torch.float16)

    _compiled_kernel = bwd_kernel[grid](
        a, b, out,
        grad_a, grad_b, grad_out
        # todo-now: it seems to have 9 arguments, so subtracing actaul poitner inputs = 6 inputs left. So seems other scalar inputs got hardcoded?
        # M, N, K,
        # a.stride(0), a.stride(1),
        # b.stride(0), b.stride(1),
        # upstream.stride(0), upstream.stride(1),

        # BLOCK_SIZE_M,
        # BLOCK_SIZE_N,
        # BLOCK_SIZE_K,
    )
    return _compiled_kernel, grad_a, grad_b


np_a = np.array([
    [0.5895, 0.8242, 0.8224, 0.4423, 0.5992, 0.3458, 0.2245, 0.8821, 0.9563, 0.9836, 0.1330, 0.1090, 0.3287, 0.2598, 0.9146, 0.0404, 0.9174, 0.4485, 0.1659, 0.5473, 0.5647, 0.7443, 0.2049, 0.6707, 0.8652, 0.3262, 0.8804, 0.0712, 0.7384, 0.1969, 0.0092, 0.1306],
    [0.3594, 0.4947, 0.5911, 0.2900, 0.4653, 0.2498, 0.8918, 0.0114, 0.6590, 0.8579, 0.6068, 0.5822, 0.0330, 0.7991, 0.9855, 0.1587, 0.2382, 0.1710, 0.5041, 0.7949, 0.9219, 0.8359, 0.8435, 0.9795, 0.7914, 0.0141, 0.5588, 0.6987, 0.2973, 0.9363, 0.9274, 0.2683],
    [0.8801, 0.9528, 0.6831, 0.7817, 0.5176, 0.1359, 0.5276, 0.8681, 0.6727, 0.9760, 0.4927, 0.0870, 0.3319, 0.8757, 0.0563, 0.8198, 0.7978, 0.4407, 0.5235, 0.0654, 0.5486, 0.4584, 0.9241, 0.3305, 0.2587, 0.7689, 0.2105, 0.0293, 0.7302, 0.7408, 0.8788, 0.1099],
    [0.1861, 0.3583, 0.3183, 0.7489, 0.1417, 0.3796, 0.2766, 0.1038, 0.2048, 0.9968, 0.0945, 0.9352, 0.0970, 0.4305, 0.0118, 0.1696, 0.9104, 0.6641, 0.1034, 0.9573, 0.1971, 0.4801, 0.1019, 0.0618, 0.1718, 0.2351, 0.1160, 0.3910, 0.9379, 0.1183, 0.9358, 0.7830],
    [0.2408, 0.8196, 0.9706, 0.6975, 0.1864, 0.8253, 0.8333, 0.9553, 0.8541, 0.6001, 0.4985, 0.6822, 0.0809, 0.4204, 0.4191, 0.3094, 0.7267, 0.2191, 0.6396, 0.1337, 0.3113, 0.4962, 0.5854, 0.3114, 0.6834, 0.9576, 0.1047, 0.5540, 0.3448, 0.6734, 0.6547, 0.4380],
    [0.1366, 0.7415, 0.7529, 0.9507, 0.5744, 0.6418, 0.6873, 0.4985, 0.0496, 0.9774, 0.8752, 0.9790, 0.6881, 0.5926, 0.8109, 0.6544, 0.5065, 0.7698, 0.7884, 0.5830, 0.3192, 0.9799, 0.2201, 0.2436, 0.1017, 0.2236, 0.3104, 0.3647, 0.4079, 0.3626, 0.9793, 0.8398],
    [0.5808, 0.1640, 0.4511, 0.2335, 0.6868, 0.8549, 0.5630, 0.8763, 0.7343, 0.5495, 0.9250, 0.4928, 0.9976, 0.5513, 0.4884, 0.6012, 0.6832, 0.1806, 0.6678, 0.4250, 0.3865, 0.4722, 0.2983, 0.9398, 0.4936, 0.8885, 0.4170, 0.4497, 0.7246, 0.4774, 0.2208, 0.4121],
    [0.6840, 0.2034, 0.8108, 0.2633, 0.2871, 0.4744, 0.6086, 0.0593, 0.9651, 0.0168, 0.1496, 0.0521, 0.5163, 0.6368, 0.8080, 0.8535, 0.9014, 0.6374, 0.0799, 0.4318, 0.1695, 0.0137, 0.2812, 0.8226, 0.3434, 0.9965, 0.2699, 0.5108, 0.7875, 0.4081, 0.4193, 0.5445],
    [0.1066, 0.0880, 0.7639, 0.1473, 0.2326, 0.0957, 0.6144, 0.3739, 0.6974, 0.4842, 0.5025, 0.3264, 0.0977, 0.8727, 0.6230, 0.5786, 0.5783, 0.7933, 0.1800, 0.7513, 0.6846, 0.2136, 0.6357, 0.0615, 0.0767, 0.0041, 0.8983, 0.5731, 0.9846, 0.8214, 0.7833, 0.3836],
    [0.2623, 0.4144, 0.6810, 0.4074, 0.0538, 0.2389, 0.9998, 0.4164, 0.8946, 0.9315, 0.9031, 0.9791, 0.5562, 0.3520, 0.9121, 0.1363, 0.1929, 0.8430, 0.4654, 0.3759, 0.8576, 0.8291, 0.1131, 0.7911, 0.0270, 0.2396, 0.5382, 0.8720, 0.9564, 0.9799, 0.9496, 0.4601],
    [0.4191, 0.5231, 0.2691, 0.0615, 0.2295, 0.4208, 0.0023, 0.6614, 0.0584, 0.3070, 0.5889, 0.1057, 0.3980, 0.2492, 0.6667, 0.5435, 0.5610, 0.6399, 0.7490, 0.6894, 0.6142, 0.2151, 0.2496, 0.3628, 0.5979, 0.6034, 0.2264, 0.9764, 0.8040, 0.0251, 0.7577, 0.8480],
    [0.6150, 0.8405, 0.2959, 0.6296, 0.6392, 0.6847, 0.9952, 0.6634, 0.2198, 0.2959, 0.1110, 0.3435, 0.4397, 0.8968, 0.5820, 0.5442, 0.5883, 0.1376, 0.2631, 0.4289, 0.9168, 0.3605, 0.3189, 0.6106, 0.0849, 0.6715, 0.5879, 0.9701, 0.0775, 0.8290, 0.6436, 0.7320],
    [0.9446, 0.6277, 0.6657, 0.9476, 0.6202, 0.9031, 0.7377, 0.6371, 0.1944, 0.0464, 0.5277, 0.7005, 0.4564, 0.3829, 0.0148, 0.5973, 0.0127, 0.6317, 0.4504, 0.8840, 0.4431, 0.5762, 0.7831, 0.3350, 0.6051, 0.6971, 0.4536, 0.3554, 0.2914, 0.7932, 0.6228, 0.5177],
    [0.0359, 0.6940, 0.8647, 0.8897, 0.6288, 0.0267, 0.7506, 0.3407, 0.2332, 0.9667, 0.7723, 0.3518, 0.1681, 0.6705, 0.6709, 0.7890, 0.4714, 0.1227, 0.5159, 0.3126, 0.2013, 0.6080, 0.4812, 0.2414, 0.3760, 0.1093, 0.6491, 0.7711, 0.3803, 0.3001, 0.5822, 0.8553],
    [0.5767, 0.1507, 0.7306, 0.2883, 0.6830, 0.9651, 0.2992, 0.0409, 0.0135, 0.5640, 0.4033, 0.6376, 0.3718, 0.7693, 0.5122, 0.5648, 0.2393, 0.8946, 0.6447, 0.9190, 0.9871, 0.6887, 0.9139, 0.8462, 0.4874, 0.4374, 0.3754, 0.9469, 0.1251, 0.9209, 0.7145, 0.2504],
    [0.9555, 0.6256, 0.2291, 0.3134, 0.6869, 0.8333, 0.0280, 0.1282, 0.5544, 0.2593, 0.2579, 0.3210, 0.5422, 0.6429, 0.8708, 0.1016, 0.3889, 0.2040, 0.1596, 0.6210, 0.7181, 0.6019, 0.0923, 0.9394, 0.8039, 0.4737, 0.0026, 0.0320, 0.7961, 0.8113, 0.3901, 0.7368],
    [0.1011, 0.4840, 0.8173, 0.2713, 0.4523, 0.4315, 0.6140, 0.5099, 0.9179, 0.6011, 0.3638, 0.1175, 0.3407, 0.7467, 0.1651, 0.1957, 0.3349, 0.6928, 0.5112, 0.7934, 0.4476, 0.2931, 0.4437, 0.5498, 0.3221, 0.5206, 0.8590, 0.9643, 0.1913, 0.6922, 0.4023, 0.0447],
    [0.1110, 0.6675, 0.6540, 0.3455, 0.6253, 0.1114, 0.6129, 0.7975, 0.4421, 0.5100, 0.2760, 0.9000, 0.8155, 0.2702, 0.0434, 0.9981, 0.9581, 0.9640, 0.8863, 0.4837, 0.1807, 0.2556, 0.8645, 0.9759, 0.0559, 0.7458, 0.7750, 0.6376, 0.2897, 0.2191, 0.8518, 0.8625],
    [0.7174, 0.4088, 0.9106, 0.0056, 0.9309, 0.2750, 0.5668, 0.6419, 0.0524, 0.3610, 0.2731, 0.1010, 0.1306, 0.0910, 0.2067, 0.5391, 0.6081, 0.1608, 0.7836, 0.8005, 0.2093, 0.7705, 0.7365, 0.8868, 0.6071, 0.3154, 0.0385, 0.5009, 0.3021, 0.7342, 0.9139, 0.8632],
    [0.4592, 0.0578, 0.4623, 0.6035, 0.2135, 0.9777, 0.2034, 0.9585, 0.7771, 0.6607, 0.6287, 0.5318, 0.8733, 0.0737, 0.8182, 0.2513, 0.0040, 0.1895, 0.9508, 0.0283, 0.9606, 0.7418, 0.0851, 0.8468, 0.0646, 0.0321, 0.0685, 0.0456, 0.2956, 0.7275, 0.8074, 0.6055],
    [0.0342, 0.3154, 0.7915, 0.3401, 0.3045, 0.0541, 0.2489, 0.6919, 0.2066, 0.0567, 0.0034, 0.0674, 0.3342, 0.6279, 0.8394, 0.1836, 0.9373, 0.2620, 0.5421, 0.7972, 0.3354, 0.5406, 0.6493, 0.8951, 0.7488, 0.2819, 0.4608, 0.6974, 0.8338, 0.8332, 0.3706, 0.6300],
    [0.1039, 0.7809, 0.0051, 0.4721, 0.0619, 0.4226, 0.0132, 0.5507, 0.8573, 0.2065, 0.3817, 0.7513, 0.3606, 0.1170, 0.0691, 0.4140, 0.5713, 0.9245, 0.5307, 0.4595, 0.2921, 0.1209, 0.6867, 0.1206, 0.1999, 0.1074, 0.7939, 0.3659, 0.7180, 0.0272, 0.5721, 0.3933],
    [0.8710, 0.2990, 0.1659, 0.8866, 0.0143, 0.0192, 0.1338, 0.3611, 0.0611, 0.3832, 0.7872, 0.9490, 0.9915, 0.1889, 0.5706, 0.1222, 0.8254, 0.4737, 0.4043, 0.9510, 0.0395, 0.2027, 0.9319, 0.7673, 0.3393, 0.4929, 0.5780, 0.2229, 0.6180, 0.3189, 0.5220, 0.5010],
    [0.0552, 0.4754, 0.6823, 0.5194, 0.2216, 0.8410, 0.6067, 0.9741, 0.7795, 0.8870, 0.7500, 0.6907, 0.8138, 0.2323, 0.7941, 0.9492, 0.9946, 0.2622, 0.0423, 0.1746, 0.4784, 0.5965, 0.4097, 0.2065, 0.5922, 0.4320, 0.9864, 0.3180, 0.9280, 0.4388, 0.4627, 0.7404],
    [0.2831, 0.8458, 0.5787, 0.1438, 0.5496, 0.9281, 0.1790, 0.4346, 0.7159, 0.6460, 0.1252, 0.6798, 0.9012, 0.1344, 0.9797, 0.3477, 0.7377, 0.0475, 0.2999, 0.2274, 0.7234, 0.5593, 0.5483, 0.2445, 0.5142, 0.4594, 0.1634, 0.7152, 0.0361, 0.4432, 0.7386, 0.1524],
    [0.5155, 0.3184, 0.9636, 0.1216, 0.9707, 0.6761, 0.0803, 0.7935, 0.7057, 0.0512, 0.2106, 0.7021, 0.3807, 0.1115, 0.8287, 0.6664, 0.2475, 0.8692, 0.6247, 0.3500, 0.1116, 0.8274, 0.5420, 0.2154, 0.7273, 0.3826, 0.4940, 0.4621, 0.8039, 0.1767, 0.1202, 0.4846],
    [0.7132, 0.5212, 0.8921, 0.4443, 0.8713, 0.3861, 0.5113, 0.4966, 0.5538, 0.5278, 0.9535, 0.9766, 0.8239, 0.6862, 0.9974, 0.3890, 0.9176, 0.9569, 0.5502, 0.1876, 0.7045, 0.0044, 0.6192, 0.5813, 0.9757, 0.7537, 0.0494, 0.6735, 0.2008, 0.2267, 0.7204, 0.9767],
    [0.5207, 0.8120, 0.2961, 0.9540, 0.7934, 0.5202, 0.7043, 0.1769, 0.4587, 0.7154, 0.0257, 0.5145, 0.6761, 0.8900, 0.5252, 0.4476, 0.2356, 0.8790, 0.5406, 0.2982, 0.5245, 0.2461, 0.7948, 0.5395, 0.5397, 0.6980, 0.9254, 0.2196, 0.7866, 0.1103, 0.9089, 0.9201],
    [0.3010, 0.3681, 0.8902, 0.7896, 0.5374, 0.3693, 0.7767, 0.1309, 0.7627, 0.5542, 0.2572, 0.3744, 0.7021, 0.8005, 0.7594, 0.8875, 0.5332, 0.0433, 0.6331, 0.1095, 0.8963, 0.0576, 0.9240, 0.5850, 0.7273, 0.0583, 0.3530, 0.4622, 0.2023, 0.4308, 0.3784, 0.0463],
    [0.2310, 0.5300, 0.5389, 0.5027, 0.4837, 0.8752, 0.7016, 0.0920, 0.6953, 0.7660, 0.7445, 0.4370, 0.6470, 0.6559, 0.2491, 0.2012, 0.8732, 0.7665, 0.4961, 0.2267, 0.0367, 0.5788, 0.5084, 0.0270, 0.3882, 0.2199, 0.8181, 0.5382, 0.1955, 0.5471, 0.2002, 0.2575],
    [0.6319, 0.1825, 0.2172, 0.0988, 0.0564, 0.0699, 0.6129, 0.0236, 0.5744, 0.2686, 0.5645, 0.8972, 0.8324, 0.9019, 0.2082, 0.4264, 0.2843, 0.4392, 0.2569, 0.2289, 0.9057, 0.2620, 0.6857, 0.8471, 0.9209, 0.8933, 0.6357, 0.2681, 0.2553, 0.0535, 0.4553, 0.4614],
    [0.9531, 0.3803, 0.9833, 0.8220, 0.1668, 0.7563, 0.0489, 0.9906, 0.8145, 0.6725, 0.0276, 0.1662, 0.7196, 0.5571, 0.4345, 0.0529, 0.3023, 0.3685, 0.5988, 0.1073, 0.6608, 0.0294, 0.4945, 0.7837, 0.7296, 0.8103, 0.5476, 0.1815, 0.3823, 0.7564, 0.9546, 0.3330],
])

np_b = np.array([
    [0.6288, 0.0270, 0.6730, 0.4112, 0.5497, 0.4425, 0.4452, 0.1217, 0.4993, 0.9791, 0.8940, 0.0742, 0.0306, 0.6236, 0.8222, 0.5738, 0.8569, 0.6784, 0.4555, 0.9945, 0.2031, 0.0985, 0.5972, 0.3435, 0.7821, 0.0761, 0.7012, 0.6099, 0.6738, 0.2407, 0.7702, 0.4790],
    [0.4619, 0.3858, 0.9638, 0.6272, 0.1690, 0.8663, 0.7114, 0.9607, 0.7270, 0.1951, 0.7024, 0.9389, 0.4842, 0.9116, 0.7406, 0.8817, 0.5882, 0.1857, 0.5606, 0.2080, 0.0874, 0.5376, 0.4095, 0.6816, 0.1099, 0.3472, 0.3395, 0.1272, 0.2224, 0.2881, 0.6606, 0.3475],
    [0.4885, 0.1943, 0.8927, 0.5323, 0.0332, 0.1003, 0.4139, 0.3690, 0.5447, 0.9950, 0.5684, 0.3578, 0.7351, 0.4494, 0.9073, 0.1171, 0.8586, 0.8041, 0.5904, 0.9331, 0.0433, 0.8322, 0.4495, 0.8785, 0.4854, 0.9663, 0.4617, 0.8099, 0.0887, 0.8877, 0.1195, 0.3844],
    [0.2290, 0.6892, 0.5470, 0.3461, 0.8078, 0.0869, 0.6533, 0.5916, 0.9887, 0.2340, 0.1240, 0.2872, 0.9171, 0.2687, 0.2461, 0.9602, 0.8111, 0.8270, 0.2123, 0.7497, 0.1884, 0.5541, 0.0985, 0.2792, 0.1503, 0.1531, 0.4462, 0.4992, 0.9732, 0.0765, 0.3413, 0.8676],
    [0.8741, 0.0706, 0.0236, 0.6293, 0.1812, 0.1792, 0.3304, 0.8419, 0.7119, 0.2609, 0.1860, 0.0897, 0.5357, 0.1085, 0.5845, 0.1329, 0.5283, 0.3199, 0.6670, 0.7726, 0.3075, 0.0763, 0.6563, 0.0672, 0.3702, 0.4598, 0.7725, 0.4366, 0.5730, 0.4495, 0.5435, 0.8494],
    [0.5426, 0.8943, 0.4368, 0.2459, 0.8284, 0.2457, 0.1859, 0.2175, 0.4140, 0.7287, 0.6234, 0.5910, 0.4576, 0.1987, 0.2315, 0.8112, 0.5238, 0.7846, 0.2911, 0.2542, 0.0681, 0.0627, 0.4650, 0.6191, 0.1080, 0.9626, 0.7844, 0.2482, 0.3028, 0.5315, 0.5256, 0.2124],
    [0.4811, 0.7848, 0.0140, 0.4435, 0.9429, 0.7578, 0.5824, 0.7621, 0.1417, 0.3712, 0.2085, 0.4223, 0.1907, 0.2633, 0.7459, 0.7478, 0.1622, 0.0031, 0.7919, 0.4278, 0.6076, 0.1446, 0.0962, 0.4889, 0.1303, 0.1587, 0.9411, 0.7549, 0.5037, 0.8785, 0.9055, 0.2846],
    [0.1694, 0.9455, 0.8403, 0.4559, 0.8441, 0.1792, 0.5810, 0.5158, 0.6507, 0.3593, 0.0416, 0.9374, 0.8730, 0.7599, 0.2479, 0.3715, 0.0440, 0.1311, 0.5517, 0.8629, 0.8502, 0.1251, 0.3458, 0.5511, 0.6075, 0.4973, 0.8022, 0.1856, 0.3191, 0.9742, 0.2273, 0.9494],
    [0.0949, 0.1345, 0.8655, 0.5466, 0.5982, 0.2007, 0.8104, 0.6632, 0.4887, 0.7691, 0.7655, 0.9205, 0.7806, 0.8402, 0.8011, 0.0835, 0.3014, 0.3505, 0.7906, 0.5184, 0.2672, 0.8266, 0.1525, 0.1343, 0.8500, 0.7120, 0.7361, 0.5654, 0.4986, 0.3303, 0.2072, 0.7880],
    [0.6063, 0.0027, 0.0770, 0.8031, 0.0916, 0.9303, 0.9449, 0.5598, 0.1659, 0.8851, 0.0338, 0.4956, 0.4822, 0.9847, 0.7993, 0.8189, 0.0079, 0.3233, 0.7157, 0.9918, 0.1642, 0.3152, 0.3308, 0.8305, 0.8856, 0.1415, 0.2370, 0.6442, 0.5017, 0.0507, 0.1119, 0.2109],
    [0.2711, 0.8424, 0.2394, 0.1855, 0.8225, 0.0791, 0.6425, 0.2562, 0.5708, 0.8450, 0.8972, 0.9892, 0.9489, 0.8993, 0.7949, 0.6141, 0.3344, 0.7179, 0.5113, 0.3492, 0.4346, 0.3176, 0.5566, 0.6085, 0.6622, 0.2862, 0.7224, 0.9812, 0.3526, 0.2465, 0.0990, 0.9943],
    [0.6722, 0.0940, 0.3008, 0.5066, 0.5959, 0.2325, 0.9216, 0.8804, 0.5560, 0.2533, 0.1397, 0.5220, 0.2217, 0.7294, 0.6376, 0.2640, 0.0660, 0.4125, 0.8407, 0.2810, 0.2519, 0.4249, 0.6052, 0.6097, 0.8777, 0.3585, 0.1723, 0.2475, 0.4492, 0.0140, 0.2708, 0.7122],
    [0.8450, 0.4855, 0.6604, 0.0651, 0.4770, 0.0153, 0.2859, 0.1093, 0.6782, 0.0060, 0.8905, 0.0586, 0.0590, 0.1708, 0.4397, 0.5226, 0.5843, 0.4116, 0.1173, 0.9697, 0.7820, 0.2938, 0.2304, 0.7635, 0.8653, 0.6753, 0.7100, 0.0295, 0.4418, 0.6234, 0.4820, 0.3563],
    [0.5306, 0.4890, 0.4470, 0.0561, 0.0673, 0.2218, 0.0717, 0.9784, 0.6267, 0.3910, 0.9939, 0.6817, 0.4387, 0.7084, 0.5835, 0.5271, 0.5936, 0.6443, 0.5915, 0.1897, 0.1032, 0.1551, 0.7178, 0.5055, 0.8197, 0.9457, 0.1200, 0.8158, 0.1682, 0.2301, 0.0952, 0.6456],
    [0.3208, 0.9734, 0.7432, 0.3151, 0.8775, 0.1190, 0.4606, 0.6407, 0.1452, 0.9371, 0.7799, 0.5235, 0.9438, 0.8951, 0.3754, 0.4102, 0.4963, 0.2177, 0.8575, 0.4252, 0.4530, 0.2606, 0.4479, 0.1473, 0.3140, 0.0450, 0.6839, 0.8696, 0.8256, 0.8845, 0.8863, 0.8263],
    [0.8815, 0.9066, 0.4574, 0.1022, 0.3868, 0.7560, 0.2300, 0.6321, 0.0972, 0.6945, 0.5208, 0.6048, 0.5514, 0.6379, 0.4413, 0.9979, 0.0166, 0.1513, 0.0046, 0.6633, 0.0420, 0.3722, 0.4001, 0.8641, 0.4197, 0.1733, 0.1720, 0.2951, 0.4210, 0.3558, 0.8903, 0.9560],
    [0.0412, 0.9154, 0.3715, 0.9167, 0.0042, 0.3496, 0.4861, 0.6294, 0.5876, 0.3068, 0.4630, 0.2024, 0.7479, 0.0554, 0.3997, 0.0920, 0.9596, 0.1518, 0.4079, 0.9221, 0.5805, 0.9369, 0.8422, 0.6913, 0.5335, 0.0196, 0.9111, 0.0723, 0.0937, 0.1800, 0.2777, 0.7132],
    [0.4277, 0.6856, 0.2795, 0.9175, 0.3305, 0.2892, 0.0759, 0.1522, 0.6564, 0.7321, 0.4091, 0.0065, 0.7941, 0.7168, 0.7470, 0.1941, 0.8169, 0.5650, 0.6140, 0.1199, 0.0104, 0.6762, 0.5400, 0.4163, 0.2715, 0.0846, 0.8046, 0.0043, 0.8653, 0.7839, 0.3732, 0.6926],
    [0.2936, 0.8057, 0.6862, 0.0488, 0.3329, 0.1797, 0.6338, 0.7488, 0.6368, 0.9107, 0.2456, 0.0952, 0.6735, 0.2675, 0.7083, 0.6538, 0.5753, 0.0939, 0.7175, 0.6871, 0.2818, 0.7709, 0.0397, 0.3546, 0.0887, 0.6582, 0.4477, 0.6097, 0.2968, 0.0196, 0.4788, 0.7582],
    [0.1468, 0.5058, 0.8212, 0.4536, 0.2605, 0.8026, 0.7920, 0.6596, 0.3137, 0.3931, 0.6573, 0.6184, 0.4816, 0.9717, 0.7941, 0.1474, 0.9904, 0.6994, 0.7883, 0.1729, 0.0027, 0.9315, 0.7380, 0.2782, 0.2012, 0.4024, 0.4185, 0.7473, 0.3491, 0.0842, 0.5322, 0.5907],
    [0.5587, 0.0910, 0.2894, 0.4605, 0.9563, 0.5742, 0.9536, 0.0151, 0.1039, 0.2053, 0.2177, 0.1467, 0.2643, 0.3776, 0.4693, 0.0688, 0.6012, 0.3966, 0.5424, 0.2865, 0.4146, 0.5237, 0.8313, 0.3394, 0.0013, 0.9530, 0.0425, 0.4694, 0.3050, 0.1371, 0.5732, 0.5989],
    [0.8556, 0.3130, 0.9404, 0.0032, 0.2140, 0.2489, 0.1819, 0.5050, 0.3663, 0.3842, 0.1393, 0.5022, 0.3938, 0.5075, 0.6791, 0.6232, 0.6280, 0.3349, 0.0708, 0.2598, 0.5415, 0.6481, 0.0503, 0.1844, 0.8806, 0.9473, 0.4642, 0.4002, 0.1698, 0.7607, 0.5809, 0.5092],
    [0.5208, 0.7192, 0.8746, 0.3399, 0.6242, 0.9719, 0.1565, 0.2083, 0.6007, 0.7827, 0.2396, 0.5809, 0.2327, 0.1523, 0.0139, 0.5306, 0.7740, 0.8556, 0.9090, 0.7952, 0.4010, 0.0347, 0.1303, 0.2530, 0.2849, 0.5881, 0.7251, 0.6676, 0.6561, 0.8303, 0.0217, 0.0820],
    [0.4365, 0.3061, 0.7785, 0.8707, 0.7561, 0.8721, 0.9368, 0.9682, 0.0669, 0.6044, 0.9928, 0.0325, 0.1487, 0.0562, 0.8951, 0.4050, 0.4687, 0.6481, 0.6243, 0.3307, 0.6325, 0.7316, 0.5804, 0.2461, 0.8727, 0.0837, 0.7700, 0.7197, 0.4520, 0.7952, 0.0959, 0.7379],
    [0.6567, 0.2268, 0.5729, 0.9218, 0.4713, 0.9971, 0.4200, 0.4311, 0.1877, 0.5622, 0.9722, 0.9711, 0.6805, 0.9315, 0.7447, 0.3307, 0.0763, 0.2649, 0.7046, 0.6722, 0.3645, 0.6436, 0.5428, 0.0769, 0.2414, 0.0694, 0.6986, 0.7633, 0.3585, 0.3618, 0.2897, 0.9725],
    [0.9611, 0.1827, 0.1819, 0.4606, 0.6642, 0.0947, 0.3238, 0.2166, 0.5132, 0.5130, 0.4171, 0.5762, 0.5267, 0.3181, 0.5459, 0.2008, 0.5389, 0.7879, 0.4616, 0.4987, 0.2437, 0.8444, 0.0168, 0.6475, 0.4964, 0.4321, 0.0058, 0.0711, 0.6937, 0.4592, 0.4400, 0.7927],
    [0.6608, 0.5087, 0.5181, 0.6999, 0.4160, 0.7175, 0.2838, 0.1151, 0.9189, 0.3010, 0.4223, 0.4559, 0.2828, 0.8344, 0.2221, 0.0950, 0.1896, 0.4509, 0.5769, 0.0673, 0.1441, 0.1270, 0.0530, 0.8715, 0.5641, 0.2106, 0.6987, 0.3324, 0.5517, 0.9137, 0.6758, 0.7319],
    [0.4638, 0.6960, 0.8114, 0.1456, 0.5406, 0.2348, 0.8454, 0.5222, 0.7170, 0.7602, 0.9910, 0.2958, 0.4158, 0.3408, 0.0419, 0.6722, 0.9064, 0.0738, 0.7415, 0.4435, 0.1842, 0.6272, 0.2328, 0.0426, 0.1779, 0.3968, 0.3508, 0.5986, 0.2858, 0.3372, 0.8776, 0.3729],
    [0.0531, 0.7000, 0.4886, 0.0285, 0.0921, 0.4235, 0.8450, 0.8348, 0.8230, 0.2890, 0.5429, 0.7549, 0.6965, 0.9230, 0.8427, 0.5227, 0.1886, 0.9847, 0.5785, 0.7589, 0.2701, 0.0431, 0.7116, 0.1451, 0.7598, 0.5174, 0.4265, 0.9854, 0.9511, 0.6343, 0.4266, 0.6395],
    [0.3539, 0.9544, 0.2305, 0.7364, 0.8621, 0.4974, 0.8789, 0.7417, 0.6294, 0.8689, 0.2890, 0.6526, 0.4991, 0.3719, 0.7900, 0.7833, 0.7399, 0.8215, 0.4320, 0.1361, 0.7569, 0.3896, 0.3702, 0.9365, 0.5504, 0.4169, 0.1049, 0.7867, 0.4408, 0.8949, 0.6134, 0.0375],
    [0.5228, 0.3995, 0.8486, 0.9491, 0.0248, 0.1412, 0.6122, 0.9085, 0.4326, 0.5017, 0.8445, 0.8719, 0.8788, 0.3112, 0.8084, 0.4078, 0.4411, 0.5449, 0.7305, 0.4238, 0.1844, 0.6665, 0.0286, 0.2316, 0.3002, 0.6041, 0.0836, 0.5014, 0.9213, 0.6832, 0.5598, 0.2526],
    [0.5622, 0.9686, 0.9480, 0.3356, 0.4195, 0.3998, 0.1505, 0.9228, 0.0337, 0.3695, 0.4631, 0.7716, 0.9167, 0.8643, 0.5424, 0.4358, 0.3422, 0.5909, 0.8099, 0.4850, 0.9036, 0.7596, 0.2793, 0.6553, 0.3609, 0.1550, 0.6413, 0.7663, 0.9488, 0.9386, 0.9040, 0.2714],
])


a = torch.from_numpy(np_a).to(dtype=torch.float16, device='cuda:0')
b = torch.from_numpy(np_b).to(dtype=torch.float16, device='cuda:0')

_compiled_kernel, grad_a, grad_b = bwd(a, b)
print("grad a: ", grad_a)
print("grad b: ", grad_b)
print()

# compare with pytorch
torch_a = torch.from_numpy(np_a).to(device='cuda:0')
torch_a.requires_grad = True
torch_b = torch.from_numpy(np_b).to(device='cuda:0')
torch_b.requires_grad = True


torch_output = torch_a @ torch_b
torch_output.backward(torch.ones_like(torch_output))

print("torch grad a: ", torch_a.grad)
print("torch grad b: ", torch_b.grad)
print()

print(torch.abs(grad_a - torch_a.grad).mean())
print(torch.abs(grad_a - torch_b.grad).mean())
print()



if torch.allclose(grad_a, torch_a.grad.to(dtype=torch.float16), atol=1e-2, rtol=0.02):
    print("✅ Triton and Torch match")
else:
    print("❌ Triton and Torch differ")


if torch.allclose(grad_a, torch_b.grad.to(dtype=torch.float16), atol=1e-2, rtol=0.02):
    print("✅ Triton and Torch match")
else:
    print("❌ Triton and Torch differ")