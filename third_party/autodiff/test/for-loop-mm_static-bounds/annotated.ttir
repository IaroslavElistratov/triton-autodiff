

iterating over op "tt.return"() : () -> ()
"builtin.module"() ({
  "tt.func"() <{arg_attrs = [{tt.divisibility = 16 : i32}, {tt.divisibility = 16 : i32}, {tt.divisibility = 16 : i32}, {tt.divisibility = 16 : i32}, {tt.divisibility = 16 : i32}, {tt.divisibility = 16 : i32}, {tt.divisibility = 16 : i32}, {tt.divisibility = 16 : i32}, {tt.divisibility = 16 : i32}], function_type = (!tt.ptr<f16>, !tt.ptr<f16>, !tt.ptr<f16>, i32, i32, i32, i32, i32, i32) -> (), sym_name = "matmul_kernel", sym_visibility = "public"}> ({
  ^bb0(%arg0: !tt.ptr<f16>, %arg1: !tt.ptr<f16>, %arg2: !tt.ptr<f16>, %arg3: i32, %arg4: i32, %arg5: i32, %arg6: i32, %arg7: i32, %arg8: i32):
    %0 = "tt.splat"(%arg2) {autogradVisited = true, isCloned = true} : (!tt.ptr<f16>) -> tensor<16x1x!tt.ptr<f16>>
    %1 = "tt.splat"(%arg8) {autogradVisited = true, isCloned = true} : (i32) -> tensor<16x1xi32>
    %2 = "tt.get_program_id"() <{axis = 0 : i32}> {autogradVisited = true, isCloned = true} : () -> i32
    %3 = "arith.constant"() <{value = 15 : i32}> {autogradVisited = true, isCloned = true} : () -> i32
    %4 = "arith.addi"(%arg4, %3) <{overflowFlags = #arith.overflow<none>}> {autogradVisited = true, isCloned = true} : (i32, i32) -> i32
    %5 = "arith.constant"() <{value = 16 : i32}> {autogradVisited = true, isCloned = true} : () -> i32
    %6 = "arith.divsi"(%4, %5) {autogradVisited = true, isCloned = true} : (i32, i32) -> i32
    %7 = "arith.divsi"(%2, %6) {autogradVisited = true, isCloned = true} : (i32, i32) -> i32
    %8 = "arith.muli"(%7, %5) <{overflowFlags = #arith.overflow<none>}> {autogradVisited = true, isCloned = true} : (i32, i32) -> i32
    %9 = "tt.splat"(%8) {autogradVisited = true, isCloned = true} : (i32) -> tensor<16xi32>
    %10 = "tt.make_range"() <{end = 16 : i32, start = 0 : i32}> {autogradVisited = true, isCloned = true} : () -> tensor<16xi32>
    %11 = "arith.addi"(%9, %10) <{overflowFlags = #arith.overflow<none>}> {autogradVisited = true, isCloned = true} : (tensor<16xi32>, tensor<16xi32>) -> tensor<16xi32>
    %12 = "tt.expand_dims"(%11) <{axis = 1 : i32}> {autogradVisited = true, isCloned = true} : (tensor<16xi32>) -> tensor<16x1xi32>
    %13 = "arith.muli"(%1, %12) <{overflowFlags = #arith.overflow<none>}> {autogradVisited = true, isCloned = true} : (tensor<16x1xi32>, tensor<16x1xi32>) -> tensor<16x1xi32>
    %14 = "tt.addptr"(%0, %13) {autogradVisited = true, isCloned = true} : (tensor<16x1x!tt.ptr<f16>>, tensor<16x1xi32>) -> tensor<16x1x!tt.ptr<f16>>
    %15 = "tt.broadcast"(%14) {autogradVisited = true, isCloned = true} : (tensor<16x1x!tt.ptr<f16>>) -> tensor<16x16x!tt.ptr<f16>>
    %16 = "arith.remsi"(%2, %6) {autogradVisited = true, isCloned = true} : (i32, i32) -> i32
    %17 = "arith.muli"(%16, %5) <{overflowFlags = #arith.overflow<none>}> {autogradVisited = true, isCloned = true} : (i32, i32) -> i32
    %18 = "tt.splat"(%17) {autogradVisited = true, isCloned = true} : (i32) -> tensor<16xi32>
    %19 = "arith.addi"(%18, %10) <{overflowFlags = #arith.overflow<none>}> {autogradVisited = true, isCloned = true} : (tensor<16xi32>, tensor<16xi32>) -> tensor<16xi32>
    %20 = "tt.expand_dims"(%19) <{axis = 0 : i32}> {autogradVisited = true, isCloned = true} : (tensor<16xi32>) -> tensor<1x16xi32>
    %21 = "tt.broadcast"(%20) {autogradVisited = true, isCloned = true} : (tensor<1x16xi32>) -> tensor<16x16xi32>
    %22 = "tt.addptr"(%15, %21) {autogradVisited = true, isCloned = true} : (tensor<16x16x!tt.ptr<f16>>, tensor<16x16xi32>) -> tensor<16x16x!tt.ptr<f16>>
    %23 = "tt.splat"(%arg0) {autogradVisited = true, isCloned = true} : (!tt.ptr<f16>) -> tensor<16x16x!tt.ptr<f16>>
    %24 = "tt.splat"(%arg3) {autogradVisited = true, isCloned = true} : (i32) -> tensor<16xi32>
    %25 = "arith.remsi"(%11, %24) {autogradVisited = true, isCloned = true} : (tensor<16xi32>, tensor<16xi32>) -> tensor<16xi32>
    %26 = "tt.expand_dims"(%25) <{axis = 1 : i32}> {autogradVisited = true, isCloned = true} : (tensor<16xi32>) -> tensor<16x1xi32>
    %27 = "tt.splat"(%arg6) {autogradVisited = true, isCloned = true} : (i32) -> tensor<16x1xi32>
    %28 = "arith.muli"(%26, %27) <{overflowFlags = #arith.overflow<none>}> {autogradVisited = true, isCloned = true} : (tensor<16x1xi32>, tensor<16x1xi32>) -> tensor<16x1xi32>
    %29 = "tt.broadcast"(%28) {autogradVisited = true, isCloned = true} : (tensor<16x1xi32>) -> tensor<16x16xi32>
    %30 = "tt.expand_dims"(%10) <{axis = 0 : i32}> {autogradVisited = true, isCloned = true} : (tensor<16xi32>) -> tensor<1x16xi32>
    %31 = "tt.broadcast"(%30) {autogradVisited = true, isCloned = true} : (tensor<1x16xi32>) -> tensor<16x16xi32>
    %32 = "arith.addi"(%29, %31) <{overflowFlags = #arith.overflow<none>}> {autogradVisited = true, isCloned = true} : (tensor<16x16xi32>, tensor<16x16xi32>) -> tensor<16x16xi32>
    %33 = "tt.addptr"(%23, %32) {autogradVisited = true, isCloned = true} : (tensor<16x16x!tt.ptr<f16>>, tensor<16x16xi32>) -> tensor<16x16x!tt.ptr<f16>>
    %34 = "arith.constant"() <{value = dense<16> : tensor<16x16xi32>}> {autogradVisited = true, isCloned = true} : () -> tensor<16x16xi32>
    %35 = "tt.addptr"(%33, %34) {autogradVisited = true, isCloned = true} : (tensor<16x16x!tt.ptr<f16>>, tensor<16x16xi32>) -> tensor<16x16x!tt.ptr<f16>>
    %36 = "tt.load"(%35) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32, isVolatile = false, operandSegmentSizes = array<i32: 1, 0, 0>}> {autogradVisited = true, isCloned = true} : (tensor<16x16x!tt.ptr<f16>>) -> tensor<16x16xf16>
    %37 = "tt.splat"(%arg1) {autogradVisited = true, isCloned = true} : (!tt.ptr<f16>) -> tensor<16x16x!tt.ptr<f16>>
    %38 = "tt.expand_dims"(%10) <{axis = 1 : i32}> {autogradVisited = true, isCloned = true} : (tensor<16xi32>) -> tensor<16x1xi32>
    %39 = "tt.splat"(%arg7) {autogradVisited = true, isCloned = true} : (i32) -> tensor<16x1xi32>
    %40 = "arith.muli"(%38, %39) <{overflowFlags = #arith.overflow<none>}> {autogradVisited = true, isCloned = true} : (tensor<16x1xi32>, tensor<16x1xi32>) -> tensor<16x1xi32>
    %41 = "tt.broadcast"(%40) {autogradVisited = true, isCloned = true} : (tensor<16x1xi32>) -> tensor<16x16xi32>
    %42 = "tt.splat"(%arg4) {autogradVisited = true, isCloned = true} : (i32) -> tensor<16xi32>
    %43 = "arith.remsi"(%19, %42) {autogradVisited = true, isCloned = true} : (tensor<16xi32>, tensor<16xi32>) -> tensor<16xi32>
    %44 = "tt.expand_dims"(%43) <{axis = 0 : i32}> {autogradVisited = true, isCloned = true} : (tensor<16xi32>) -> tensor<1x16xi32>
    %45 = "tt.broadcast"(%44) {autogradVisited = true, isCloned = true} : (tensor<1x16xi32>) -> tensor<16x16xi32>
    %46 = "arith.addi"(%41, %45) <{overflowFlags = #arith.overflow<none>}> {autogradVisited = true, isCloned = true} : (tensor<16x16xi32>, tensor<16x16xi32>) -> tensor<16x16xi32>
    %47 = "tt.addptr"(%37, %46) {autogradVisited = true, isCloned = true} : (tensor<16x16x!tt.ptr<f16>>, tensor<16x16xi32>) -> tensor<16x16x!tt.ptr<f16>>
    %48 = "arith.muli"(%arg7, %5) <{overflowFlags = #arith.overflow<none>}> {autogradVisited = true, isCloned = true} : (i32, i32) -> i32
    %49 = "tt.splat"(%48) {autogradVisited = true, isCloned = true} : (i32) -> tensor<16x16xi32>
    %50 = "tt.addptr"(%47, %49) {autogradVisited = true, isCloned = true} : (tensor<16x16x!tt.ptr<f16>>, tensor<16x16xi32>) -> tensor<16x16x!tt.ptr<f16>>
    %51 = "tt.load"(%50) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32, isVolatile = false, operandSegmentSizes = array<i32: 1, 0, 0>}> {autogradVisited = true, isCloned = true} : (tensor<16x16x!tt.ptr<f16>>) -> tensor<16x16xf16>
    %52 = "tt.load"(%33) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32, isVolatile = false, operandSegmentSizes = array<i32: 1, 0, 0>}> {autogradVisited = true, isCloned = true} : (tensor<16x16x!tt.ptr<f16>>) -> tensor<16x16xf16>
    %53 = "tt.load"(%47) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32, isVolatile = false, operandSegmentSizes = array<i32: 1, 0, 0>}> {autogradVisited = true, isCloned = true} : (tensor<16x16x!tt.ptr<f16>>) -> tensor<16x16xf16>

    %54 = "arith.constant"() <{value = dense<0.000000e+00> : tensor<16x16xf32>}> {autogradVisited = true, isCloned = true} : () -> tensor<16x16xf32>
    // dot 1: a(at offsets 1) @ b(at offsets 1)
    %55 = "tt.dot"(%52, %53, %54) <{inputPrecision = 0 : i32, maxNumImpreciseAcc = 0 : i32}> {autogradVisited = true, isCloned = true} : (tensor<16x16xf16>, tensor<16x16xf16>, tensor<16x16xf32>) -> tensor<16x16xf32>
    // dot 2: a(at offsets 2) @ b(at offsets offsets 2)
    %56 = "tt.dot"(%36, %51, %55) <{inputPrecision = 0 : i32, maxNumImpreciseAcc = 0 : i32}> {autogradVisited = true, isCloned = true} : (tensor<16x16xf16>, tensor<16x16xf16>, tensor<16x16xf32>) -> tensor<16x16xf32>
    %57 = "arith.truncf"(%56) {autogradVisited = true, isCloned = true} : (tensor<16x16xf32>) -> tensor<16x16xf16>

    //////////////
    // backward //
    //////////////

    // load upstream
    %58 = "tt.load"(%22) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32, isVolatile = false, operandSegmentSizes = array<i32: 1, 0, 0>}> {autogradVisited = true, isInserted = true} : (tensor<16x16x!tt.ptr<f16>>) -> tensor<16x16xf16>

    // bwd truncf
    %59 = "arith.extf"(%58) {autogradVisited = true, isInserted = true} : (tensor<16x16xf16>) -> tensor<16x16xf32>

    // b(at offsets 1).T
    %60 = "tt.trans"(%53) <{order = array<i32: 1, 0>}> {autogradVisited = true, isInserted = true} : (tensor<16x16xf16>) -> tensor<16x16xf16>
    // [this comes from my dotHandler -- accumulator for mul]
    %61 = "arith.constant"() <{value = 0.000000e+00 : f32}> {autogradVisited = true, isInserted = true} : () -> f32
    %62 = "tt.splat"(%61) {autogradVisited = true, isInserted = true} : (f32) -> tensor<16x16xf16>
    // grad wrt a (at offsets 1):   upstream @ b(at offsets 1).T
    %63 = "tt.dot"(%59, %60, %62) <{inputPrecision = 0 : i32, maxNumImpreciseAcc = 0 : i32}> {autogradVisited = true, isInserted = true} : (tensor<16x16xf32>, tensor<16x16xf16>, tensor<16x16xf16>) -> tensor<16x16xf16>

    // a(at offsets 1).T
    %64 = "tt.trans"(%52) <{order = array<i32: 1, 0>}> {autogradVisited = true, isInserted = true} : (tensor<16x16xf16>) -> tensor<16x16xf16>
    %65 = "arith.constant"() <{value = 0.000000e+00 : f32}> {autogradVisited = true, isInserted = true} : () -> f32
    %66 = "tt.splat"(%65) {autogradVisited = true, isInserted = true} : (f32) -> tensor<16x16xf16>
    // grad wrt b (at offsets 1):   a(at offsets 1).T @ upstream 
    %67 = "tt.dot"(%64, %59, %66) <{inputPrecision = 0 : i32, maxNumImpreciseAcc = 0 : i32}> {autogradVisited = true, isInserted = true} : (tensor<16x16xf16>, tensor<16x16xf32>, tensor<16x16xf16>) -> tensor<16x16xf16>

    // b(at offsets 2).T
    %68 = "tt.trans"(%51) <{order = array<i32: 1, 0>}> {autogradVisited = true, isInserted = true} : (tensor<16x16xf16>) -> tensor<16x16xf16>
    %69 = "arith.constant"() <{value = 0.000000e+00 : f32}> {autogradVisited = true, isInserted = true} : () -> f32
    %70 = "tt.splat"(%69) {autogradVisited = true, isInserted = true} : (f32) -> tensor<16x16xf16>
    // grad wrt a (at offsets 2):    upstream @ b(at offsets 2).T
    %71 = "tt.dot"(%59, %68, %70) <{inputPrecision = 0 : i32, maxNumImpreciseAcc = 0 : i32}> {autogradVisited = true, isInserted = true} : (tensor<16x16xf32>, tensor<16x16xf16>, tensor<16x16xf16>) -> tensor<16x16xf16>

    // a(at offsets 2).T
    %72 = "tt.trans"(%36) <{order = array<i32: 1, 0>}> {autogradVisited = true, isInserted = true} : (tensor<16x16xf16>) -> tensor<16x16xf16>
    %73 = "arith.constant"() <{value = 0.000000e+00 : f32}> {autogradVisited = true, isInserted = true} : () -> f32
    %74 = "tt.splat"(%73) {autogradVisited = true, isInserted = true} : (f32) -> tensor<16x16xf16>
    // grad wrt b (at offsets 2): a(at offsets 2).T @ upstream
    %75 = "tt.dot"(%72, %59, %74) <{inputPrecision = 0 : i32, maxNumImpreciseAcc = 0 : i32}> {autogradVisited = true, isInserted = true} : (tensor<16x16xf16>, tensor<16x16xf32>, tensor<16x16xf16>) -> tensor<16x16xf16>

    // todo: in the future (when same input slice is used multiple times) will need to atomic-add grads instead of store

    // sotre at b_ptr(at offsets 1)
    "tt.store"(%47, %67) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> {autogradVisited = true, gradOf = "%121 = \22tt.load\22(%116) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32, isVolatile = false, operandSegmentSizes = array<i32: 1, 0, 0>}> : (tensor<16x16x!tt.ptr<f16>>) -> tensor<16x16xf16>", isInserted = true} : (tensor<16x16x!tt.ptr<f16>>, tensor<16x16xf16>) -> ()
    // sotre at b_ptr(at offsets 2)
    "tt.store"(%50, %75) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> {autogradVisited = true, gradOf = "%126 = \22tt.load\22(%124) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32, isVolatile = false, operandSegmentSizes = array<i32: 1, 0, 0>}> : (tensor<16x16x!tt.ptr<f16>>) -> tensor<16x16xf16>", isInserted = true} : (tensor<16x16x!tt.ptr<f16>>, tensor<16x16xf16>) -> ()
    // sotre at a_ptr(at offsets 1)
    "tt.store"(%33, %63) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> {autogradVisited = true, gradOf = "%120 = \22tt.load\22(%107) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32, isVolatile = false, operandSegmentSizes = array<i32: 1, 0, 0>}> : (tensor<16x16x!tt.ptr<f16>>) -> tensor<16x16xf16>", isInserted = true} : (tensor<16x16x!tt.ptr<f16>>, tensor<16x16xf16>) -> ()
    // sotre at a_ptr(at offsets 2)
    "tt.store"(%35, %71) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> {autogradVisited = true, gradOf = "%125 = \22tt.load\22(%123) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32, isVolatile = false, operandSegmentSizes = array<i32: 1, 0, 0>}> : (tensor<16x16x!tt.ptr<f16>>) -> tensor<16x16xf16>", isInserted = true} : (tensor<16x16x!tt.ptr<f16>>, tensor<16x16xf16>) -> ()
    "tt.return"() : () -> ()
  }) {noinline = false} : () -> ()
}) : () -> ()

