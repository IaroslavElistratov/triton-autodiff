- INPUT
  module {
    tt.func public @add_kernel(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 16 : i32}) attributes {noinline = false} {
      %0 = tt.make_range {end = 4 : i32, start = 0 : i32} : tensor<4xi32>

      %1 = tt.splat %arg0 : !tt.ptr<f32> -> tensor<4x!tt.ptr<f32>>
      %2 = tt.addptr %1, %0 : tensor<4x!tt.ptr<f32>>, tensor<4xi32>
      %3 = tt.load %2 : tensor<4x!tt.ptr<f32>>

      %4 = tt.splat %arg1 : !tt.ptr<f32> -> tensor<4x!tt.ptr<f32>>
      %5 = tt.addptr %4, %0 : tensor<4x!tt.ptr<f32>>, tensor<4xi32>
      %6 = tt.load %5 : tensor<4x!tt.ptr<f32>>

      %7 = arith.divf %3, %6 : tensor<4xf32>

      %8 = tt.splat %arg2 : !tt.ptr<f32> -> tensor<4x!tt.ptr<f32>>
      %9 = tt.addptr %8, %0 : tensor<4x!tt.ptr<f32>>, tensor<4xi32>
      tt.store %9, %7 : tensor<4x!tt.ptr<f32>>
      tt.return
    }
  }


- OUTPUT
  module {
    tt.func public @add_kernel(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 16 : i32}) attributes {noinline = false} {
      %0 = tt.make_range {autogradVisited = true, end = 4 : i32, isOrig = true, start = 0 : i32} : tensor<4xi32>

      // load upstream
      %1 = tt.splat %arg2 {autogradVisited = true, isOrig = true} : !tt.ptr<f32> -> tensor<4x!tt.ptr<f32>>
      %2 = tt.addptr %1, %0 {autogradVisited = true, isOrig = true} : tensor<4x!tt.ptr<f32>>, tensor<4xi32>
      %3 = tt.load %2 {autogradVisited = true, isInserted = true} : tensor<4x!tt.ptr<f32>>

      // TODO: DON'T CLONE IF INPUTS TO AN OP YOU MATCHED TO ARE DIRECTLY INPUTS TO THE FN
      %4 = tt.splat %arg0 {autogradVisited = true, isOrig = true} : !tt.ptr<f32> -> tensor<4x!tt.ptr<f32>>
      %5 = tt.addptr %4, %0 {autogradVisited = true, isOrig = true} : tensor<4x!tt.ptr<f32>>, tensor<4xi32>

      // load a
      %6 = tt.splat %arg0 {autogradVisited = true, isCloned = true} : !tt.ptr<f32> -> tensor<4x!tt.ptr<f32>>
      %7 = tt.make_range {autogradVisited = true, end = 4 : i32, isCloned = true, isOrig = true, start = 0 : i32} : tensor<4xi32>
      %8 = tt.addptr %6, %7 {autogradVisited = true, isCloned = true} : tensor<4x!tt.ptr<f32>>, tensor<4xi32>
      %9 = tt.load %8 {autogradVisited = true, isCloned = true} : tensor<4x!tt.ptr<f32>>

      // TODO: DON'T CLONE IF INPUTS TO AN OP YOU MATCHED TO ARE DIRECTLY INPUTS TO THE FN
      %10 = tt.splat %arg1 {autogradVisited = true, isOrig = true} : !tt.ptr<f32> -> tensor<4x!tt.ptr<f32>>
      %11 = tt.addptr %10, %0 {autogradVisited = true, isOrig = true} : tensor<4x!tt.ptr<f32>>, tensor<4xi32>

      // load b
      %12 = tt.splat %arg1 {autogradVisited = true, isCloned = true} : !tt.ptr<f32> -> tensor<4x!tt.ptr<f32>>
      %13 = tt.addptr %12, %0 {autogradVisited = true, isCloned = true} : tensor<4x!tt.ptr<f32>>, tensor<4xi32>
      %14 = tt.load %13 {autogradVisited = true, isCloned = true} : tensor<4x!tt.ptr<f32>>

      // ones
      %cst = arith.constant {autogradVisited = true, isInserted = true} 1.000000e+00 : f32
      %15 = tt.splat %cst {autogradVisited = true, isInserted = true} : f32 -> tensor<4xf32>
      // a_local
      %16 = arith.divf %15, %14 {autogradVisited = true, isInserted = true} : tensor<4xf32>
      // a_grad
      %17 = arith.mulf %16, %3 {autogradVisited = true, isInserted = true} : tensor<4xf32>

      // pow
      %18 = arith.mulf %14, %14 {autogradVisited = true, isInserted = true} : tensor<4xf32>
      // div
      %19 = arith.divf %9, %18 {autogradVisited = true, isInserted = true} : tensor<4xf32>
      // neg
      %cst_0 = arith.constant {autogradVisited = true, isInserted = true} -1.000000e+00 : f32
      %20 = tt.splat %cst_0 {autogradVisited = true, isInserted = true} : f32 -> tensor<4xf32>
      // b_local
      %21 = arith.mulf %20, %18 {autogradVisited = true, isInserted = true} : tensor<4xf32>
      // b_grad
      %22 = arith.mulf %21, %3 {autogradVisited = true, isInserted = true} : tensor<4xf32>

      // store grads
      tt.store %11, %22 {autogradVisited = true, gradOf = "%24 = tt.load %12 : tensor<4x!tt.ptr<f32>>", isInserted = true} : tensor<4x!tt.ptr<f32>>
      tt.store %5, %17 {autogradVisited = true, gradOf = "%10 = tt.load %5 : tensor<4x!tt.ptr<f32>>", isInserted = true} : tensor<4x!tt.ptr<f32>>
      tt.return
    }
  }

