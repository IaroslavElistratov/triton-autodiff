^bb0(%arg0: !tt.ptr<f32>, %arg1: !tt.ptr<f32>):
  %c1_i32 = arith.constant 1 : i32
  %c3_i32 = arith.constant 3 : i32
  %c0_i32 = arith.constant 0 : i32
  %0 = tt.make_range {end = 4 : i32, start = 0 : i32} : tensor<4xi32>
  %1 = tt.splat %arg0 : !tt.ptr<f32> -> tensor<4x!tt.ptr<f32>>
  %2 = tt.addptr %1, %0 : tensor<4x!tt.ptr<f32>>, tensor<4xi32>
  %3 = tt.load %2 : tensor<4x!tt.ptr<f32>>
  %4 = tt.splat %arg1 : !tt.ptr<f32> -> tensor<4x!tt.ptr<f32>>
  %5 = tt.addptr %4, %0 : tensor<4x!tt.ptr<f32>>, tensor<4xi32>
  %6 = tt.load %5 : tensor<4x!tt.ptr<f32>>
  %c3_i32_0 = arith.constant 3 : i32
  %7 = arith.sitofp %c0_i32 : i32 to f32
  %8 = tt.splat %7 : f32 -> tensor<4xf32>
  %9 = arith.mulf %3, %8 : tensor<4xf32>
  %10 = arith.addf %6, %9 : tensor<4xf32>
  %c1_i32_1 = arith.constant 1 : i32
  %11 = arith.muli %c1_i32, %c1_i32_1 : i32
  %12 = arith.addi %c0_i32, %11 : i32
  %13 = arith.sitofp %12 : i32 to f32
  %14 = tt.splat %13 : f32 -> tensor<4xf32>
  %15 = arith.mulf %3, %14 : tensor<4xf32>
  %16 = arith.addf %10, %15 : tensor<4xf32>
  %c2_i32 = arith.constant 2 : i32
  %17 = arith.muli %c1_i32, %c2_i32 : i32
  %18 = arith.addi %c0_i32, %17 : i32
  %19 = arith.sitofp %18 : i32 to f32
  %20 = tt.splat %19 : f32 -> tensor<4xf32>
  %21 = arith.mulf %3, %20 : tensor<4xf32>
  %22 = arith.addf %16, %21 : tensor<4xf32>
  tt.store %5, %22 : tensor<4x!tt.ptr<f32>>
  tt.return



module {
  tt.func public @add_kernel(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32}) attributes {noinline = false} {
    %1 = tt.make_range {autogradVisited = true, end = 4 : i32, isCloned = true, start = 0 : i32} : tensor<4xi32>

    // load accum
    %0 = tt.splat %arg1 {autogradVisited = true, isCloned = true} : !tt.ptr<f32> -> tensor<4x!tt.ptr<f32>>
    %2 = tt.addptr %0, %1 {autogradVisited = true, isCloned = true} : tensor<4x!tt.ptr<f32>>, tensor<4xi32>
    %3 = tt.load %2 {autogradVisited = true, isCloned = true} : tensor<4x!tt.ptr<f32>>

    // load a
    %4 = tt.splat %arg0 {autogradVisited = true, isCloned = true} : !tt.ptr<f32> -> tensor<4x!tt.ptr<f32>>
    %5 = tt.addptr %4, %1 {autogradVisited = true, isCloned = true} : tensor<4x!tt.ptr<f32>>, tensor<4xi32>
    %6 = tt.load %5 {autogradVisited = true, isCloned = true} : tensor<4x!tt.ptr<f32>>

    // loop start idx?
    %c0_i32 = arith.constant {autogradVisited = true, isCloned = true} 0 : i32

    // step size?
    %c1_i32 = arith.constant {autogradVisited = true, isCloned = true} 1 : i32

    ////// iter 0 //////

    %7 = arith.sitofp %c0_i32 {autogradVisited = true, isCloned = true} : i32 to f32
    %8 = tt.splat %7 {autogradVisited = true, isCloned = true} : f32 -> tensor<4xf32>
    %9 = arith.mulf %6, %8 {autogradVisited = true, isCloned = true} : tensor<4xf32>
    %10 = arith.addf %3, %9 {autogradVisited = true, isCloned = true} : tensor<4xf32>

    ////// iter 1 //////

    / * setup loop idxs * /

    // induction variable for iteration 1
    %c1_i32_0 = arith.constant {autogradVisited = true, isCloned = true} 1 : i32
    // multiply induction variable by step size
    %11 = arith.muli %c1_i32, %c1_i32_0 {autogradVisited = true, isCloned = true} : i32
    // add to start idx
    %12 = arith.addi %c0_i32, %11 {autogradVisited = true, isCloned = true} : i32

    / * actual loop body * /

    %13 = arith.sitofp %12 {autogradVisited = true, isCloned = true} : i32 to f32
    %14 = tt.splat %13 {autogradVisited = true, isCloned = true} : f32 -> tensor<4xf32>
    %15 = arith.mulf %6, %14 {autogradVisited = true, isCloned = true} : tensor<4xf32>
    %16 = arith.addf %10, %15 {autogradVisited = true, isCloned = true} : tensor<4xf32>

    ////// iter 2 //////

    / * setup loop idxs * /

    // induction variable for iteration 2
    %c2_i32 = arith.constant {autogradVisited = true, isCloned = true} 2 : i32
    // multiply induction variable by step size
    %17 = arith.muli %c1_i32, %c2_i32 {autogradVisited = true, isCloned = true} : i32
    // add to start idx
    %18 = arith.addi %c0_i32, %17 {autogradVisited = true, isCloned = true} : i32

    / * actual loop body * /

    %19 = arith.sitofp %18 {autogradVisited = true, isCloned = true} : i32 to f32
    %20 = tt.splat %19 {autogradVisited = true, isCloned = true} : f32 -> tensor<4xf32>
    %21 = arith.mulf %6, %20 {autogradVisited = true, isCloned = true} : tensor<4xf32>
    %22 = arith.addf %16, %21 {autogradVisited = true, isCloned = true} : tensor<4xf32>

    /////////////////
    /// backward ///
    ////////////////

    // load upstream
    %23 = tt.load %2 {autogradVisited = true, isInserted = true} : tensor<4x!tt.ptr<f32>>

    // grad accum
    %24 = arith.mulf %6, %23 {autogradVisited = true, isInserted = true} : tensor<4xf32>
    // grad a
    %25 = arith.mulf %8, %23 {autogradVisited = true, isInserted = true} : tensor<4xf32>

    // grad accum
    %26 = arith.mulf %6, %23 {autogradVisited = true, isInserted = true} : tensor<4xf32>
    // grad a
    %27 = arith.mulf %14, %23 {autogradVisited = true, isInserted = true} : tensor<4xf32>

    // grad accum
    %28 = arith.mulf %6, %23 {autogradVisited = true, isInserted = true} : tensor<4xf32>
    // grad a
    %29 = arith.mulf %20, %23 {autogradVisited = true, isInserted = true} : tensor<4xf32>

    // sum up all grad of a
    %30 = arith.addf %29, %27 {autogradVisited = true, isInserted = true} : tensor<4xf32>
    %31 = arith.addf %30, %25 {autogradVisited = true, isInserted = true} : tensor<4xf32>

    // todo: it writes the same thing that it loaded (upstream) -- useless
    tt.store %2, %23 {autogradVisited = true, gradOf = "%38 = tt.load %37 : tensor<4x!tt.ptr<f32>>", isInserted = true} : tensor<4x!tt.ptr<f32>>

    // store grad a
    tt.store %5, %31 {autogradVisited = true, gradOf = "%35 = tt.load %34 : tensor<4x!tt.ptr<f32>>", isInserted = true} : tensor<4x!tt.ptr<f32>>

    tt.return
  }
}

