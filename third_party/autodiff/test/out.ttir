module {
  tt.func public @add_kernel(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 16 : i32}) attributes {noinline = false} {
    %0 = tt.make_range {autogradVisited = true, end = 4 : i32, isOrig = true, start = 0 : i32} : tensor<4xi32>
    %1 = tt.splat %arg2 {autogradVisited = true, isOrig = true} : !tt.ptr<f32> -> tensor<4x!tt.ptr<f32>>
    %2 = tt.addptr %1, %0 {autogradVisited = true, isOrig = true} : tensor<4x!tt.ptr<f32>>, tensor<4xi32>
    %3 = tt.load %2 {autogradVisited = true, isInserted = true} : tensor<4x!tt.ptr<f32>>
    %4 = tt.splat %arg0 {autogradVisited = true, isOrig = true} : !tt.ptr<f32> -> tensor<4x!tt.ptr<f32>>
    %5 = tt.addptr %4, %0 {autogradVisited = true, isOrig = true} : tensor<4x!tt.ptr<f32>>, tensor<4xi32>
    %6 = tt.splat %arg0 {autogradVisited = true, isCloned = true} : !tt.ptr<f32> -> tensor<4x!tt.ptr<f32>>
    %7 = tt.make_range {autogradVisited = true, end = 4 : i32, isCloned = true, start = 0 : i32} : tensor<4xi32>
    %8 = tt.addptr %6, %7 {autogradVisited = true, isCloned = true} : tensor<4x!tt.ptr<f32>>, tensor<4xi32>
    %9 = tt.load %8 {autogradVisited = true, isCloned = true} : tensor<4x!tt.ptr<f32>>
    %10 = tt.splat %arg1 {autogradVisited = true, isOrig = true} : !tt.ptr<f32> -> tensor<4x!tt.ptr<f32>>
    %11 = tt.addptr %10, %0 {autogradVisited = true, isOrig = true} : tensor<4x!tt.ptr<f32>>, tensor<4xi32>
    %12 = tt.splat %arg1 {autogradVisited = true, isCloned = true} : !tt.ptr<f32> -> tensor<4x!tt.ptr<f32>>
    %13 = tt.make_range {autogradVisited = true, end = 4 : i32, isCloned = true, start = 0 : i32} : tensor<4xi32>
    %14 = tt.addptr %12, %13 {autogradVisited = true, isCloned = true} : tensor<4x!tt.ptr<f32>>, tensor<4xi32>
    %15 = tt.load %14 {autogradVisited = true, isCloned = true} : tensor<4x!tt.ptr<f32>>
    %cst = arith.constant {autogradVisited = true, isInserted = true} 1.000000e+00 : f32
    %16 = tt.splat %cst {autogradVisited = true, isInserted = true} : f32 -> tensor<4xf32>
    %17 = arith.divf %16, %15 {autogradVisited = true, isInserted = true} : tensor<4xf32>
    %18 = arith.mulf %17, %3 {autogradVisited = true, isInserted = true} : tensor<4xf32>
    %19 = arith.mulf %15, %15 {autogradVisited = true, isInserted = true} : tensor<4xf32>
    %20 = arith.divf %9, %19 {autogradVisited = true, isInserted = true} : tensor<4xf32>
    %cst_0 = arith.constant {autogradVisited = true, isInserted = true} -1.000000e+00 : f32
    %21 = tt.splat %cst_0 {autogradVisited = true, isInserted = true} : f32 -> tensor<4xf32>
    %22 = arith.mulf %21, %20 {autogradVisited = true, isInserted = true} : tensor<4xf32>
    %23 = arith.mulf %22, %3 {autogradVisited = true, isInserted = true} : tensor<4xf32>
    tt.store %11, %23 {autogradVisited = true, gradOf = "%25 = tt.load %12 : tensor<4x!tt.ptr<f32>>", isInserted = true} : tensor<4x!tt.ptr<f32>>
    tt.store %5, %18 {autogradVisited = true, gradOf = "%10 = tt.load %5 : tensor<4x!tt.ptr<f32>>", isInserted = true} : tensor<4x!tt.ptr<f32>>
    tt.return
  }
}

