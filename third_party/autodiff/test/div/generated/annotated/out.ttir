module {
  tt.func public @add_kernel(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg3: !tt.ptr<f32>, %arg4: !tt.ptr<f32>, %arg5: !tt.ptr<f32>) attributes {noinline = false} {
    // get pointer for c
    %0 = tt.splat %arg2 {autogradVisited = true, isCloned = true} : !tt.ptr<f32> -> tensor<4x!tt.ptr<f32>>
    %1 = tt.make_range {autogradVisited = true, end = 4 : i32, isCloned = true, start = 0 : i32} : tensor<4xi32>
    %2 = tt.addptr %0, %1 {autogradVisited = true, isCloned = true} : tensor<4x!tt.ptr<f32>>, tensor<4xi32>

    // load a
    %3 = tt.splat %arg0 {autogradVisited = true, isCloned = true} : !tt.ptr<f32> -> tensor<4x!tt.ptr<f32>>
    %4 = tt.addptr %3, %1 {autogradVisited = true, isCloned = true} : tensor<4x!tt.ptr<f32>>, tensor<4xi32>
    %5 = tt.load %4 {autogradVisited = true, isCloned = true} : tensor<4x!tt.ptr<f32>>

    // load b
    %6 = tt.splat %arg1 {autogradVisited = true, isCloned = true} : !tt.ptr<f32> -> tensor<4x!tt.ptr<f32>>
    %7 = tt.addptr %6, %1 {autogradVisited = true, isCloned = true} : tensor<4x!tt.ptr<f32>>, tensor<4xi32>
    %8 = tt.load %7 {autogradVisited = true, isCloned = true} : tensor<4x!tt.ptr<f32>>

    %9 = arith.divf %5, %8 {autogradVisited = true, isCloned = true} : tensor<4xf32>

    // *******************
    // **** backward *****
    // *******************

    // load upstream
    //  1) replay transformations (that were applied to out_ptr) on top of grad_out_ptr
    %10 = tt.splat %arg5 {autogradVisited = true, isGradPtrRebase = true} : !tt.ptr<f32> -> tensor<4x!tt.ptr<f32>>
    %11 = tt.addptr %10, %1 {autogradVisited = true, isGradPtrRebase = true} : tensor<4x!tt.ptr<f32>>, tensor<4xi32>
    //  2) load
    %12 = tt.load %11 {autogradVisited = true, isInserted = true} : tensor<4x!tt.ptr<f32>>

    // ones
    %cst = arith.constant {autogradVisited = true, isInserted = true} 1.000000e+00 : f32
    %13 = tt.splat %cst {autogradVisited = true, isInserted = true} : f32 -> tensor<4xf32>
    // a local
    %14 = arith.divf %13, %8 {autogradVisited = true, isInserted = true} : tensor<4xf32>
    // a downstream
    %15 = arith.mulf %14, %12 {autogradVisited = true, isInserted = true} : tensor<4xf32>

    // pow
    %16 = arith.mulf %8, %8 {autogradVisited = true, isInserted = true} : tensor<4xf32>
    // div
    %17 = arith.divf %5, %16 {autogradVisited = true, isInserted = true} : tensor<4xf32>
    // neg
    %cst_0 = arith.constant {autogradVisited = true, isInserted = true} -1.000000e+00 : f32
    %18 = tt.splat %cst_0 {autogradVisited = true, isInserted = true} : f32 -> tensor<4xf32>
    // b local
    %19 = arith.mulf %18, %17 {autogradVisited = true, isInserted = true} : tensor<4xf32>
    // b downstream
    %20 = arith.mulf %19, %12 {autogradVisited = true, isInserted = true} : tensor<4xf32>

    // store b downstream
    //  1) apply transformations (which were originally applied to b_ptr) on top of b_grad_ptr
    %21 = tt.splat %arg4 {autogradVisited = true, isGradPtrRebase = true} : !tt.ptr<f32> -> tensor<4x!tt.ptr<f32>>
    %22 = tt.addptr %21, %1 {autogradVisited = true, isGradPtrRebase = true} : tensor<4x!tt.ptr<f32>>, tensor<4xi32>
    //  2) store grad
    %23 = tt.atomic_rmw fadd, acq_rel, gpu, %22, %20 {autogradVisited = true, gradOf = "%27 = tt.load %26 : tensor<4x!tt.ptr<f32>>", isInserted = true} : (tensor<4x!tt.ptr<f32>>, tensor<4xf32>) -> tensor<4xf32>

    // store a downstream
    //  1) apply transformations (which were originally applied to a_ptr) on top of a_grad_ptr
    %24 = tt.splat %arg3 {autogradVisited = true, isGradPtrRebase = true} : !tt.ptr<f32> -> tensor<4x!tt.ptr<f32>>
    %25 = tt.addptr %24, %1 {autogradVisited = true, isGradPtrRebase = true} : tensor<4x!tt.ptr<f32>>, tensor<4xi32>
    //  2) store grad
    %26 = tt.atomic_rmw fadd, acq_rel, gpu, %25, %15 {autogradVisited = true, gradOf = "%24 = tt.load %23 : tensor<4x!tt.ptr<f32>>", isInserted = true} : (tensor<4x!tt.ptr<f32>>, tensor<4xf32>) -> tensor<4xf32>
    tt.return
  }
}

