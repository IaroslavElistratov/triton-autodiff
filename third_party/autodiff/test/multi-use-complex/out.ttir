module {
  tt.func public @kernel(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f32>, %arg3: !tt.ptr<f32>) attributes {noinline = false} {
    %0 = tt.splat %arg1 {autogradVisited = true, isCloned = true} : !tt.ptr<f32> -> tensor<4x!tt.ptr<f32>>
    %1 = tt.make_range {autogradVisited = true, end = 4 : i32, isCloned = true, start = 0 : i32} : tensor<4xi32>
    %2 = tt.addptr %0, %1 {autogradVisited = true, isCloned = true} : tensor<4x!tt.ptr<f32>>, tensor<4xi32>
    %3 = tt.splat %arg0 {autogradVisited = true, isCloned = true} : !tt.ptr<f32> -> tensor<4x!tt.ptr<f32>>
    %4 = tt.addptr %3, %1 {autogradVisited = true, isCloned = true} : tensor<4x!tt.ptr<f32>>, tensor<4xi32>
    %5 = tt.load %4 {autogradVisited = true, isCloned = true} : tensor<4x!tt.ptr<f32>>
    %cst = arith.constant {autogradVisited = true, isCloned = true} dense<5.000000e-01> : tensor<4xf32>

    // x = a + 0.5
    %6 = arith.addf %5, %cst {autogradVisited = true, isCloned = true} : tensor<4xf32>
    // y = a * x
    %7 = arith.mulf %5, %6 {autogradVisited = true, isCloned = true} : tensor<4xf32>
    // z = a * y
    %8 = arith.mulf %5, %7 {autogradVisited = true, isCloned = true} : tensor<4xf32>


    //////////////
    // backward //
    //////////////

    // load upstream -- replay same transformations that were applied to the out_ptr, not apply them on the grad_out_ptr
    %9 = tt.splat %arg3 {autogradVisited = true, isGradPtrRebase = true} : !tt.ptr<f32> -> tensor<4x!tt.ptr<f32>>
    %10 = tt.addptr %9, %1 {autogradVisited = true, isGradPtrRebase = true} : tensor<4x!tt.ptr<f32>>, tensor<4xi32>
    %11 = tt.load %10 {autogradVisited = true, isInserted = true} : tensor<4x!tt.ptr<f32>>

    /// backprop through Z: ///
    // y_grad = a * upstream
    %12 = arith.mulf %5, %11 {autogradVisited = true, isInserted = true} : tensor<4xf32>
    // a_grad = y * upstream
    %15 = arith.mulf %7, %11 {autogradVisited = true, isInserted = true} : tensor<4xf32>

    /// backprop through Y: ///
    // x_grad = a * y_grad (upstream wrt y)
    %13 = arith.mulf %5, %12 {autogradVisited = true, isInserted = true} : tensor<4xf32>
    // a_grad = x * y_grad (upstream wrt y)
    %14 = arith.mulf %6, %12 {autogradVisited = true, isInserted = true} : tensor<4xf32>

    /// backprop through X: ///
    // NOTE: omitted, bc local grad of ADD is 1 -- omit multiplying by 1
    // [a_grad = x_grad]

    // NOTE: accumulate the two grads of a
    %16 = arith.addf %15, %14 {autogradVisited = true, isInserted = true} : tensor<4xf32>
    // and accumulate its final grad (from the x node)
    %17 = arith.addf %16, %13 {autogradVisited = true, isInserted = true} : tensor<4xf32>

    // store accumulated grad of a, in grad_a_ptr
    //  1) replay transformations (which were originally applied to a_ptr) on top of grad_a_ptr
    %18 = tt.splat %arg2 {autogradVisited = true, isGradPtrRebase = true} : !tt.ptr<f32> -> tensor<4x!tt.ptr<f32>>
    %19 = tt.addptr %18, %1 {autogradVisited = true, isGradPtrRebase = true} : tensor<4x!tt.ptr<f32>>, tensor<4xi32>
    // todo: this load is not needed
    %20 = tt.load %19 {autogradVisited = true, isGradPtrRebase = true} : tensor<4x!tt.ptr<f32>>
    //  2) store grad
    %21 = tt.atomic_rmw fadd, acq_rel, gpu, %19, %17 {autogradVisited = true, gradOf = "%21 = tt.load %20 : tensor<4x!tt.ptr<f32>>", isInserted = true} : (tensor<4x!tt.ptr<f32>>, tensor<4xf32>) -> tensor<4xf32>
    tt.return
  }
}

