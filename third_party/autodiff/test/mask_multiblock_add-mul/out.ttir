module {
  tt.func public @add_kernel(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 16 : i32}) attributes {noinline = false} {
    %0 = tt.splat %arg2 {autogradVisited = true, isCloned = true} : !tt.ptr<f32> -> tensor<4x!tt.ptr<f32>>
    %1 = tt.get_program_id x {autogradVisited = true, isCloned = true} : i32
    %c4_i32 = arith.constant {autogradVisited = true, isCloned = true} 4 : i32
    %2 = arith.muli %1, %c4_i32 {autogradVisited = true, isCloned = true} : i32
    %3 = tt.splat %2 {autogradVisited = true, isCloned = true} : i32 -> tensor<4xi32>
    %4 = tt.make_range {autogradVisited = true, end = 4 : i32, isCloned = true, start = 0 : i32} : tensor<4xi32>
    %5 = arith.addi %3, %4 {autogradVisited = true, isCloned = true} : tensor<4xi32>
    %6 = tt.addptr %0, %5 {autogradVisited = true, isCloned = true} : tensor<4x!tt.ptr<f32>>, tensor<4xi32>
    %7 = tt.splat %arg0 {autogradVisited = true, isCloned = true} : !tt.ptr<f32> -> tensor<4x!tt.ptr<f32>>
    %8 = tt.addptr %7, %5 {autogradVisited = true, isCloned = true} : tensor<4x!tt.ptr<f32>>, tensor<4xi32>
    %cst = arith.constant {autogradVisited = true, isCloned = true} dense<10> : tensor<4xi32>
    %9 = arith.cmpi slt, %5, %cst {autogradVisited = true, isCloned = true} : tensor<4xi32>
    %10 = tt.load %8, %9 {autogradVisited = true, isCloned = true} : tensor<4x!tt.ptr<f32>>
    %cst_0 = arith.constant {autogradVisited = true, isCloned = true} dense<5.000000e-01> : tensor<4xf32>
    %11 = arith.addf %10, %cst_0 {autogradVisited = true, isCloned = true} : tensor<4xf32>
    %12 = tt.splat %arg1 {autogradVisited = true, isCloned = true} : !tt.ptr<f32> -> tensor<4x!tt.ptr<f32>>
    %13 = tt.addptr %12, %5 {autogradVisited = true, isCloned = true} : tensor<4x!tt.ptr<f32>>, tensor<4xi32>
    %14 = tt.load %13, %9 {autogradVisited = true, isCloned = true} : tensor<4x!tt.ptr<f32>>
    %15 = arith.mulf %11, %14 {autogradVisited = true, isCloned = true} : tensor<4xf32>

    //////////////
    // backward //
    //////////////

    // masked load upstream
    %16 = tt.load %6, %9 {autogradVisited = true, isInserted = true} : tensor<4x!tt.ptr<f32>>

    // b grad: x * upstream
    %17 = arith.mulf %11, %16 {autogradVisited = true, isInserted = true} : tensor<4xf32>
    // x grad: b * upstream
    %18 = arith.mulf %14, %16 {autogradVisited = true, isInserted = true} : tensor<4xf32>

    // masked store: b grad into b ptr
    tt.store %13, %17, %9 {autogradVisited = true, gradOf = "%30 = tt.load %29, %24 : tensor<4x!tt.ptr<f32>>", isInserted = true} : tensor<4x!tt.ptr<f32>>
    // masked store: x grad into a ptr (skipping multiplication by 1, for add bwd)
    tt.store %8, %18, %9 {autogradVisited = true, gradOf = "%27 = tt.load %26, %24 : tensor<4x!tt.ptr<f32>>", isInserted = true} : tensor<4x!tt.ptr<f32>>

    tt.return
  }
}

