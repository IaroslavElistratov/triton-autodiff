module {
  tt.func public @add_kernel(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg3: !tt.ptr<f32>, %arg4: !tt.ptr<f32>, %arg5: !tt.ptr<f32>) attributes {noinline = false} {

    // block_start = pid * BLOCK_SIZE
    %1 = tt.get_program_id x {autogradVisited = true, isCloned = true} : i32
    %c4_i32 = arith.constant {autogradVisited = true, isCloned = true} 4 : i32
    %2 = arith.muli %1, %c4_i32 {autogradVisited = true, isCloned = true} : i32

    // offsets = block_start + tl.arange(0, BLOCK_SIZE)
    %3 = tt.splat %2 {autogradVisited = true, isCloned = true} : i32 -> tensor<4xi32>
    %4 = tt.make_range {autogradVisited = true, end = 4 : i32, isCloned = true, start = 0 : i32} : tensor<4xi32>
    %5 = arith.addi %3, %4 {autogradVisited = true, isCloned = true} : tensor<4xi32>

    // todo: this was used in the fwd to STORE the output
    //  of the fwd -- but bc I'm deleting that STORE from FWD, as to
    //  not overwrite the upstream grad, these 2 ops are not used in FWD part of this graph
    //  ==> they are however used in the bwd part
    %0 = tt.splat %arg2 {autogradVisited = true, isCloned = true} : !tt.ptr<f32> -> tensor<4x!tt.ptr<f32>>
    %6 = tt.addptr %0, %5 {autogradVisited = true, isCloned = true} : tensor<4x!tt.ptr<f32>>, tensor<4xi32>

    // load a
    %7 = tt.splat %arg0 {autogradVisited = true, isCloned = true} : !tt.ptr<f32> -> tensor<4x!tt.ptr<f32>>
    %8 = tt.addptr %7, %5 {autogradVisited = true, isCloned = true} : tensor<4x!tt.ptr<f32>>, tensor<4xi32>
    %9 = tt.load %8 {autogradVisited = true, isCloned = true} : tensor<4x!tt.ptr<f32>>

    // x = a + 0.5
    %cst = arith.constant {autogradVisited = true, isCloned = true} dense<5.000000e-01> : tensor<4xf32>
    %10 = arith.addf %9, %cst {autogradVisited = true, isCloned = true} : tensor<4xf32>

    // load b
    %11 = tt.splat %arg1 {autogradVisited = true, isCloned = true} : !tt.ptr<f32> -> tensor<4x!tt.ptr<f32>>
    %12 = tt.addptr %11, %5 {autogradVisited = true, isCloned = true} : tensor<4x!tt.ptr<f32>>, tensor<4xi32>
    %13 = tt.load %12 {autogradVisited = true, isCloned = true} : tensor<4x!tt.ptr<f32>>

    // y = x * b
    %14 = arith.mulf %10, %13 {autogradVisited = true, isCloned = true} : tensor<4xf32>

    //////////////
    // backward //
    //////////////

    // load upstream (replaying some transformations which were applied to out_ptr, now applied to grad_out_ptr)
    %15 = tt.splat %arg5 {autogradVisited = true, isGradPtrRebase = true} : !tt.ptr<f32> -> tensor<4x!tt.ptr<f32>>
    %16 = tt.addptr %15, %5 {autogradVisited = true, isGradPtrRebase = true} : tensor<4x!tt.ptr<f32>>, tensor<4xi32>
    %17 = tt.load %16 {autogradVisited = true, isInserted = true} : tensor<4x!tt.ptr<f32>>

    // grad b: x * upstream
    %18 = arith.mulf %10, %17 {autogradVisited = true, isInserted = true} : tensor<4xf32>
    // grad x: b * upstream
    %19 = arith.mulf %13, %17 {autogradVisited = true, isInserted = true} : tensor<4xf32>

    // store grad b at b_grad_ptr
    //  1) load b_grad (replaying transformations that were applied to b_ptr)
    %20 = tt.splat %arg4 {autogradVisited = true, isGradPtrRebase = true} : !tt.ptr<f32> -> tensor<4x!tt.ptr<f32>>
    %21 = tt.addptr %20, %5 {autogradVisited = true, isGradPtrRebase = true} : tensor<4x!tt.ptr<f32>>, tensor<4xi32>
    %22 = tt.load %21 {autogradVisited = true, isGradPtrRebase = true} : tensor<4x!tt.ptr<f32>>
    //  2) store b grad
    %23 = tt.atomic_rmw fadd, acq_rel, gpu, %21, %18 {autogradVisited = true, gradOf = "%30 = tt.load %29 : tensor<4x!tt.ptr<f32>>", isInserted = true} : (tensor<4x!tt.ptr<f32>>, tensor<4xf32>) -> tensor<4xf32>

    // store grad x at a_grad_ptr (omitting multiplying by 1 for differentiating though add)
    //  1) load a_ptr (replaying transformations that were applied to a_ptr)
    %24 = tt.splat %arg3 {autogradVisited = true, isGradPtrRebase = true} : !tt.ptr<f32> -> tensor<4x!tt.ptr<f32>>
    %25 = tt.addptr %24, %5 {autogradVisited = true, isGradPtrRebase = true} : tensor<4x!tt.ptr<f32>>, tensor<4xi32>
    %26 = tt.load %25 {autogradVisited = true, isGradPtrRebase = true} : tensor<4x!tt.ptr<f32>>
    //  2) store a grad
    %27 = tt.atomic_rmw fadd, acq_rel, gpu, %25, %19 {autogradVisited = true, gradOf = "%27 = tt.load %26 : tensor<4x!tt.ptr<f32>>", isInserted = true} : (tensor<4x!tt.ptr<f32>>, tensor<4xf32>) -> tensor<4xf32>
    tt.return
  }
}

// todo: in these sample cases atomic add is not needed