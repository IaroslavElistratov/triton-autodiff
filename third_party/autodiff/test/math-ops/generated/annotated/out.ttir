module {
  tt.func public @add_kernel(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg3: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg4: !tt.ptr<f32>, %arg5: !tt.ptr<f32>, %arg6: !tt.ptr<f32>, %arg7: !tt.ptr<f32>) attributes {noinline = false} {
    %0 = tt.splat %arg3 {autogradVisited = true, isCloned = true} : !tt.ptr<f32> -> tensor<4x!tt.ptr<f32>>
    %1 = tt.make_range {autogradVisited = true, end = 4 : i32, isCloned = true, start = 0 : i32} : tensor<4xi32>
    %2 = tt.addptr %0, %1 {autogradVisited = true, isCloned = true} : tensor<4x!tt.ptr<f32>>, tensor<4xi32>
    %3 = tt.splat %arg0 {autogradVisited = true, isCloned = true} : !tt.ptr<f32> -> tensor<4x!tt.ptr<f32>>
    %4 = tt.addptr %3, %1 {autogradVisited = true, isCloned = true} : tensor<4x!tt.ptr<f32>>, tensor<4xi32>
    %5 = tt.load %4 {autogradVisited = true, isCloned = true} : tensor<4x!tt.ptr<f32>>
    %cst = arith.constant {autogradVisited = true, isCloned = true} dense<5.000000e-01> : tensor<4xf32>
    %6 = arith.addf %5, %cst {autogradVisited = true, isCloned = true} : tensor<4xf32>
    %7 = tt.splat %arg1 {autogradVisited = true, isCloned = true} : !tt.ptr<f32> -> tensor<4x!tt.ptr<f32>>
    %8 = tt.addptr %7, %1 {autogradVisited = true, isCloned = true} : tensor<4x!tt.ptr<f32>>, tensor<4xi32>
    %9 = tt.load %8 {autogradVisited = true, isCloned = true} : tensor<4x!tt.ptr<f32>>
    %10 = arith.mulf %6, %9 {autogradVisited = true, isCloned = true} : tensor<4xf32>
    %11 = tt.splat %arg2 {autogradVisited = true, isCloned = true} : !tt.ptr<f32> -> tensor<4x!tt.ptr<f32>>
    %12 = tt.addptr %11, %1 {autogradVisited = true, isCloned = true} : tensor<4x!tt.ptr<f32>>, tensor<4xi32>
    %13 = tt.load %12 {autogradVisited = true, isCloned = true} : tensor<4x!tt.ptr<f32>>
    %14 = arith.divf %10, %13 {autogradVisited = true, isCloned = true} : tensor<4xf32>
    %15 = math.cos %14 {autogradVisited = true, isCloned = true} : tensor<4xf32>
    %16 = math.sin %15 {autogradVisited = true, isCloned = true} : tensor<4xf32>
    %17 = math.sqrt %16 {autogradVisited = true, isCloned = true} : tensor<4xf32>
    %18 = math.log %17 {autogradVisited = true, isCloned = true} : tensor<4xf32>
    %19 = math.exp %18 {autogradVisited = true, isCloned = true} : tensor<4xf32>
    %20 = tt.splat %arg7 {autogradVisited = true, isGradPtrRebase = true} : !tt.ptr<f32> -> tensor<4x!tt.ptr<f32>>
    %21 = tt.addptr %20, %1 {autogradVisited = true, isGradPtrRebase = true} : tensor<4x!tt.ptr<f32>>, tensor<4xi32>
    %22 = tt.load %21 {autogradVisited = true, isInserted = true} : tensor<4x!tt.ptr<f32>>
    %23 = arith.mulf %19, %22 {autogradVisited = true, isInserted = true} : tensor<4xf32>
    %cst_0 = arith.constant {autogradVisited = true, isInserted = true} 1.000000e+00 : f32
    %24 = tt.splat %cst_0 {autogradVisited = true, isInserted = true} : f32 -> tensor<4xf32>
    %25 = arith.divf %24, %17 {autogradVisited = true, isInserted = true} : tensor<4xf32>
    %26 = arith.mulf %25, %23 {autogradVisited = true, isInserted = true} : tensor<4xf32>
    %cst_1 = arith.constant {autogradVisited = true, isInserted = true} 2.000000e+00 : f32
    %27 = tt.splat %cst_1 {autogradVisited = true, isInserted = true} : f32 -> tensor<4xf32>
    %28 = arith.mulf %17, %27 {autogradVisited = true, isInserted = true} : tensor<4xf32>
    %cst_2 = arith.constant {autogradVisited = true, isInserted = true} 1.000000e+00 : f32
    %29 = tt.splat %cst_2 {autogradVisited = true, isInserted = true} : f32 -> tensor<4xf32>
    %30 = arith.divf %29, %28 {autogradVisited = true, isInserted = true} : tensor<4xf32>
    %31 = arith.mulf %30, %26 {autogradVisited = true, isInserted = true} : tensor<4xf32>
    %32 = math.cos %15 {autogradVisited = true, isInserted = true} : tensor<4xf32>
    %33 = arith.mulf %32, %31 {autogradVisited = true, isInserted = true} : tensor<4xf32>
    %34 = math.sin %14 {autogradVisited = true, isInserted = true} : tensor<4xf32>
    %cst_3 = arith.constant {autogradVisited = true, isInserted = true} -1.000000e+00 : f32
    %35 = tt.splat %cst_3 {autogradVisited = true, isInserted = true} : f32 -> tensor<4xf32>
    %36 = arith.mulf %35, %34 {autogradVisited = true, isInserted = true} : tensor<4xf32>
    %37 = arith.mulf %36, %33 {autogradVisited = true, isInserted = true} : tensor<4xf32>
    %cst_4 = arith.constant {autogradVisited = true, isInserted = true} 1.000000e+00 : f32
    %38 = tt.splat %cst_4 {autogradVisited = true, isInserted = true} : f32 -> tensor<4xf32>
    %39 = arith.divf %38, %13 {autogradVisited = true, isInserted = true} : tensor<4xf32>
    %40 = arith.mulf %39, %37 {autogradVisited = true, isInserted = true} : tensor<4xf32>
    %41 = arith.mulf %6, %40 {autogradVisited = true, isInserted = true} : tensor<4xf32>
    %42 = arith.mulf %9, %40 {autogradVisited = true, isInserted = true} : tensor<4xf32>
    %43 = arith.mulf %13, %13 {autogradVisited = true, isInserted = true} : tensor<4xf32>
    %44 = arith.divf %10, %43 {autogradVisited = true, isInserted = true} : tensor<4xf32>
    %cst_5 = arith.constant {autogradVisited = true, isInserted = true} -1.000000e+00 : f32
    %45 = tt.splat %cst_5 {autogradVisited = true, isInserted = true} : f32 -> tensor<4xf32>
    %46 = arith.mulf %45, %44 {autogradVisited = true, isInserted = true} : tensor<4xf32>
    %47 = arith.mulf %46, %37 {autogradVisited = true, isInserted = true} : tensor<4xf32>
    %48 = tt.splat %arg6 {autogradVisited = true, isGradPtrRebase = true} : !tt.ptr<f32> -> tensor<4x!tt.ptr<f32>>
    %49 = tt.addptr %48, %1 {autogradVisited = true, isGradPtrRebase = true} : tensor<4x!tt.ptr<f32>>, tensor<4xi32>
    %50 = tt.atomic_rmw fadd, acq_rel, gpu, %49, %47 {autogradVisited = true, gradOf = "%57 = tt.load %56 : tensor<4x!tt.ptr<f32>>", isInserted = true} : (tensor<4x!tt.ptr<f32>>, tensor<4xf32>) -> tensor<4xf32>
    %51 = tt.splat %arg5 {autogradVisited = true, isGradPtrRebase = true} : !tt.ptr<f32> -> tensor<4x!tt.ptr<f32>>
    %52 = tt.addptr %51, %1 {autogradVisited = true, isGradPtrRebase = true} : tensor<4x!tt.ptr<f32>>, tensor<4xi32>
    %53 = tt.atomic_rmw fadd, acq_rel, gpu, %52, %41 {autogradVisited = true, gradOf = "%54 = tt.load %53 : tensor<4x!tt.ptr<f32>>", isInserted = true} : (tensor<4x!tt.ptr<f32>>, tensor<4xf32>) -> tensor<4xf32>
    %54 = tt.splat %arg4 {autogradVisited = true, isGradPtrRebase = true} : !tt.ptr<f32> -> tensor<4x!tt.ptr<f32>>
    %55 = tt.addptr %54, %1 {autogradVisited = true, isGradPtrRebase = true} : tensor<4x!tt.ptr<f32>>, tensor<4xi32>
    %56 = tt.atomic_rmw fadd, acq_rel, gpu, %55, %42 {autogradVisited = true, gradOf = "%51 = tt.load %50 : tensor<4x!tt.ptr<f32>>", isInserted = true} : (tensor<4x!tt.ptr<f32>>, tensor<4xf32>) -> tensor<4xf32>
    tt.return
  }
}

